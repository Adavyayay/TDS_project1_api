{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc16913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import tiktoken\n",
    "\n",
    "BASE_URL = \"https://tds.s-anand.net/\"\n",
    "\n",
    "def fetch_sidebar():\n",
    "    \"\"\"Download the sidebar listing all course pages.\"\"\"\n",
    "    resp = requests.get(BASE_URL + \"_sidebar.md\")\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "def parse_links(markdown_text):\n",
    "    \"\"\"Extract (title, filename) pairs from Markdown links.\"\"\"\n",
    "    pattern = re.compile(r'\\[([^\\]]+)\\]\\(([^)]+)\\)')\n",
    "    return pattern.findall(markdown_text)\n",
    "\n",
    "def fetch_markdown(filename):\n",
    "    \"\"\"Download a Markdown page by filename.\"\"\"\n",
    "    url = BASE_URL + filename\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "def chunk_text(text, max_tokens=500, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Split `text` into chunks of roughly max_tokens tokens using tiktoken.\n",
    "    \"\"\"\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    token_ids = enc.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(token_ids), max_tokens):\n",
    "        chunk_ids = token_ids[i : i + max_tokens]\n",
    "        chunks.append(enc.decode(chunk_ids))\n",
    "    return chunks\n",
    "\n",
    "def main():\n",
    "    sidebar = fetch_sidebar()\n",
    "    links = parse_links(sidebar)\n",
    "\n",
    "    all_chunks = []\n",
    "    for title, filename in links:\n",
    "        md_content = fetch_markdown(filename)\n",
    "        for idx, chunk in enumerate(chunk_text(md_content)):\n",
    "            all_chunks.append({\n",
    "                \"source_url\": BASE_URL + filename,\n",
    "                \"title\": title,\n",
    "                \"chunk_index\": idx,\n",
    "                \"chunk_text\": chunk\n",
    "            })\n",
    "\n",
    "    # Write out to a JSON file\n",
    "    with open(\"tds_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8e51dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: https://discourse.onlinedegree.iitm.ac.in/categories.json",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(all_chunks, f, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 52\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     51\u001b[0m     cat_slug \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtds-kb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 52\u001b[0m     cat_id \u001b[38;5;241m=\u001b[39m \u001b[43mget_category_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_slug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     topics \u001b[38;5;241m=\u001b[39m list_topics(cat_slug, cat_id)\n\u001b[0;32m     55\u001b[0m     all_chunks \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mget_category_id\u001b[1;34m(slug)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fetch categories and return the one matching slug.\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mcategories.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m cats \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory_list\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m cats:\n",
      "File \u001b[1;32mc:\\Users\\adavy\\Desktop\\TDS_P1\\venv\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1023\u001b[0m     )\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://discourse.onlinedegree.iitm.ac.in/categories.json"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import tiktoken  # optional, for token-based chunking\n",
    "\n",
    "BASE_URL = \"https://discourse.onlinedegree.iitm.ac.in/\"\n",
    "\n",
    "def get_category_id(slug=\"tds-kb\"):\n",
    "    \"\"\"Fetch categories and return the one matching slug.\"\"\"\n",
    "    resp = requests.get(f\"{BASE_URL}categories.json\")\n",
    "    resp.raise_for_status()\n",
    "    cats = resp.json()[\"category_list\"][\"categories\"]\n",
    "    for cat in cats:\n",
    "        if cat[\"slug\"] == slug:\n",
    "            return cat[\"id\"]\n",
    "    raise ValueError(f\"Category '{slug}' not found.\")\n",
    "\n",
    "def list_topics(category_slug, category_id, page=0):\n",
    "    \"\"\"\n",
    "    Retrieve all topics for a category by paging through \n",
    "    `/c/{slug}/{id}.json` (20 topics per page).\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}c/{category_slug}/{category_id}.json?page={page}\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    topics = data[\"topic_list\"][\"topics\"]\n",
    "    # if more pages exist, recurse\n",
    "    if data[\"topic_list\"].get(\"more_topics_url\"):\n",
    "        topics += list_topics(category_slug, category_id, page + 1)\n",
    "    return topics\n",
    "\n",
    "def fetch_topic_posts(slug, topic_id):\n",
    "    \"\"\"\n",
    "    Fetch full topic JSON, including post_stream.posts array.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}t/{slug}/{topic_id}.json\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"post_stream\"][\"posts\"]\n",
    "\n",
    "def chunk_text(text, max_tokens=500, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Split text into chunks of ~max_tokens using tiktoken.\n",
    "    \"\"\"\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    tokens = enc.encode(text)\n",
    "    return [enc.decode(tokens[i : i + max_tokens]) \n",
    "            for i in range(0, len(tokens), max_tokens)]\n",
    "\n",
    "def main():\n",
    "    cat_slug = \"tds-kb\"\n",
    "    cat_id = get_category_id(cat_slug)\n",
    "    topics = list_topics(cat_slug, cat_id)\n",
    "\n",
    "    all_chunks = []\n",
    "    for topic in topics:\n",
    "        t_id = topic[\"id\"]\n",
    "        t_slug = topic[\"slug\"]\n",
    "        t_title = topic[\"title\"]\n",
    "        t_url   = f\"{BASE_URL}t/{t_slug}/{t_id}\"\n",
    "        posts = fetch_topic_posts(t_slug, t_id)\n",
    "\n",
    "        for post in posts:\n",
    "            raw = post.get(\"raw\", \"\")\n",
    "            # choose either raw or cooked\n",
    "            # Optionally split into token chunks:\n",
    "            chunks = chunk_text(raw) if raw else [\"\"]\n",
    "            for idx, text_chunk in enumerate(chunks):\n",
    "                all_chunks.append({\n",
    "                    \"topic_id\":      t_id,\n",
    "                    \"topic_slug\":    t_slug,\n",
    "                    \"topic_title\":   t_title,\n",
    "                    \"topic_url\":     t_url,\n",
    "                    \"post_id\":       post[\"id\"],\n",
    "                    \"post_number\":   post[\"post_number\"],\n",
    "                    \"username\":      post[\"username\"],\n",
    "                    \"created_at\":    post[\"created_at\"],\n",
    "                    \"chunk_index\":   idx,\n",
    "                    \"chunk_text\":    text_chunk\n",
    "                })\n",
    "\n",
    "    # Persist to disk for ingestion\n",
    "    with open(\"tds_discourse_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aad845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started.\n",
      "Fetching topic IDs from category between 2025-01-01 00:00:00+00:00 and 2025-04-15 23:59:59.999999+00:00...\n",
      "Failed to fetch page 0: 403 Client Error: Forbidden for url: https://discourse.onlinedegree.iitm.ac.in/c/courses/tds-kb/34.json?page=0\n",
      "Total unique topics found in timeframe: 0\n",
      "No topic IDs found for the given criteria. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timezone # Ensure timezone is imported\n",
    "from urllib.parse import urljoin, urlencode\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "\n",
    "DISCOURSE_BASE_URL = \"https://discourse.onlinedegree.iitm.ac.in/\"\n",
    "CATEGORY_SLUG = \"courses/tds-kb\"\n",
    "CATEGORY_ID = 34\n",
    "START_DATE = \"2025-01-01\" # Inclusive\n",
    "END_DATE = \"2025-04-15\"   # Inclusive\n",
    "\n",
    "RAW_COOKIE_STRING = \"\"\"LgekGp1zML0MGFLOfs2Jz3beYsAIU1YEFoa+yG0VJgb75S4nQlH72Pt4b5AqurCE4IevvK3RTt0iUJmL2bGUzlY4SyrIOC0AIboJtu6hw6geSVSIjRTV2CDlIXajnBC2HOMCBdTfmYKk0iR8nDoPqU+VDx04zcKF1dOQUFfqcUGrJ/H0r0D467clgxhs6eCCHtu1dueg8y7rSFHKHpCBLd8hPeYkBEQXsHeF3Ogucq3Fd2rv4HeZIPA2VYLw6uFoFhp2HuCxRwsAHxrvTWn2Fd7KB5eBrl+N/X9uANlTX7zJ1/hA64hTCV/8ZmsGX2XN--oObAbVGGnGt2bOK9--XOplwD43YQYFhtJMsw8r+g==\"\"\"\n",
    "OUTPUT_DIR = \"discourse_json\"\n",
    "POST_ID_BATCH_SIZE = 50\n",
    "MAX_CONSECUTIVE_PAGES_WITHOUT_NEW_TOPICS = 5 # New configuration for breaking loop\n",
    "\n",
    "# ====================================\n",
    "\n",
    "def parse_cookie_string(raw_cookie_string):\n",
    "    \"\"\"Parses a raw cookie string into a dictionary.\"\"\"\n",
    "    cookies = {}\n",
    "    if not raw_cookie_string.strip():\n",
    "        print(\"Warning: RAW_COOKIE_STRING is empty. Requests might fail if authentication is needed.\")\n",
    "        return cookies\n",
    "    for cookie_part in raw_cookie_string.strip().split(\";\"):\n",
    "        if \"=\" in cookie_part:\n",
    "            key, value = cookie_part.strip().split(\"=\", 1)\n",
    "            cookies[key] = value\n",
    "    return cookies\n",
    "\n",
    "\n",
    "def get_topic_ids(base_url, category_slug, category_id, start_date_str, end_date_str, cookies):\n",
    "    \"\"\"Fetches topic IDs from a specific category within a date range.\"\"\"\n",
    "    url = urljoin(base_url, f\"c/{category_slug}/{category_id}.json\")\n",
    "    topic_ids = []\n",
    "    page = 0\n",
    "\n",
    "    start_dt_naive = datetime.fromisoformat(start_date_str + \"T00:00:00\")\n",
    "    start_dt = start_dt_naive.replace(tzinfo=timezone.utc)\n",
    "    end_dt_naive = datetime.fromisoformat(end_date_str + \"T23:59:59.999999\")\n",
    "    end_dt = end_dt_naive.replace(tzinfo=timezone.utc)\n",
    "\n",
    "    print(f\"Fetching topic IDs from category between {start_dt} and {end_dt}...\")\n",
    "\n",
    "    # Variables for the new loop break condition\n",
    "    consecutive_pages_with_no_new_unique_topics = 0\n",
    "    last_known_unique_topic_count = 0\n",
    "\n",
    "    while True:\n",
    "        paginated_url = f\"{url}?page={page}\"\n",
    "        try:\n",
    "            response = requests.get(paginated_url, cookies=cookies, timeout=30)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON from page {page}. Content: {response.text[:200]}...\")\n",
    "            break\n",
    "\n",
    "        topics_on_page = data.get(\"topic_list\", {}).get(\"topics\", [])\n",
    "\n",
    "        if not topics_on_page:\n",
    "            print(f\"No more topics found on page {page} (API returned empty list).\")\n",
    "            break # Primary stop condition: API says no more topics on this page\n",
    "\n",
    "        # Store current number of unique topics before processing this page\n",
    "        # This helps check if *this specific page fetch* added anything new\n",
    "        count_before_processing_page = len(set(topic_ids))\n",
    "\n",
    "        for topic in topics_on_page:\n",
    "            created_at_str = topic.get(\"created_at\")\n",
    "            if created_at_str:\n",
    "                try:\n",
    "                    created_date = datetime.fromisoformat(created_at_str.replace(\"Z\", \"+00:00\"))\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Could not parse date '{created_at_str}' for topic ID {topic.get('id')}\")\n",
    "                    continue\n",
    "\n",
    "                if start_dt <= created_date <= end_dt:\n",
    "                    topic_ids.append(topic[\"id\"]) # Add ID, will be deduped later for count\n",
    "\n",
    "        current_unique_topic_count = len(set(topic_ids))\n",
    "\n",
    "        if topics_on_page and current_unique_topic_count == count_before_processing_page :\n",
    "            # This means the current page had topics, but none of them were new *and* within the date range,\n",
    "            # or all topics fetched from this page were duplicates of ones already in topic_ids from *previous pages*.\n",
    "            # For the staleness check, we care if the overall unique set isn't growing.\n",
    "             pass # Handled by the check below using last_known_unique_topic_count\n",
    "\n",
    "        # Staleness check: Has the *total* number of unique topics found stopped growing?\n",
    "        if current_unique_topic_count == last_known_unique_topic_count and topics_on_page:\n",
    "            # topics_on_page is checked to ensure we don't increment if an empty page was returned (which is a valid end)\n",
    "            consecutive_pages_with_no_new_unique_topics += 1\n",
    "            print(f\"Page {page} did not yield any new unique topics. Consecutive stale pages: {consecutive_pages_with_no_new_unique_topics}.\")\n",
    "        else:\n",
    "            consecutive_pages_with_no_new_unique_topics = 0 # Reset if new unique topics were found\n",
    "\n",
    "        last_known_unique_topic_count = current_unique_topic_count\n",
    "\n",
    "        if consecutive_pages_with_no_new_unique_topics >= MAX_CONSECUTIVE_PAGES_WITHOUT_NEW_TOPICS:\n",
    "            print(f\"No new unique topics found for {MAX_CONSECUTIVE_PAGES_WITHOUT_NEW_TOPICS} consecutive pages. Assuming end of relevant category listing.\")\n",
    "            break\n",
    "\n",
    "        # Original secondary stop condition (heuristic)\n",
    "        more_topics_url = data.get(\"topic_list\", {}).get(\"more_topics_url\")\n",
    "        if not more_topics_url:\n",
    "            # This typically means it's the last page.\n",
    "            # The condition `len(topics_on_page) < 30` was a heuristic for when more_topics_url might be missing\n",
    "            # but the page wasn't full. If more_topics_url is definitively gone, it's a strong signal.\n",
    "            print(f\"No 'more_topics_url' indicated on page {page}. Assuming this is the last page of topics.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Fetched page {page}, {len(topics_on_page)} topics on page. Total unique topics found so far: {current_unique_topic_count}. Continuing...\")\n",
    "        page += 1\n",
    "\n",
    "\n",
    "    final_unique_topic_ids = list(set(topic_ids)) # Deduplicate\n",
    "    print(f\"Total unique topics found in timeframe: {len(final_unique_topic_ids)}\")\n",
    "    return final_unique_topic_ids\n",
    "\n",
    "\n",
    "def get_full_topic_json(base_url, topic_id, cookies):\n",
    "    \"\"\"Fetches the full topic JSON, including all posts by handling pagination.\"\"\"\n",
    "    initial_topic_url = urljoin(base_url, f\"t/{topic_id}.json\")\n",
    "    print(f\"Fetching initial data for topic {topic_id} from {initial_topic_url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(initial_topic_url, cookies=cookies, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        topic_data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch initial topic data for {topic_id}: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to decode initial JSON for topic {topic_id}. Content: {response.text[:200]}...\")\n",
    "        return None\n",
    "\n",
    "    post_stream = topic_data.get(\"post_stream\")\n",
    "    if not post_stream or \"stream\" not in post_stream or \"posts\" not in post_stream:\n",
    "        print(f\"Error: 'post_stream' not found or incomplete in topic {topic_id}. Skipping post fetching.\")\n",
    "        return topic_data\n",
    "\n",
    "    all_post_ids_in_stream = post_stream.get(\"stream\", [])\n",
    "    loaded_post_ids = {post[\"id\"] for post in post_stream.get(\"posts\", [])}\n",
    "\n",
    "    all_post_ids_in_stream = [pid for pid in all_post_ids_in_stream if pid is not None]\n",
    "\n",
    "    missing_post_ids = [pid for pid in all_post_ids_in_stream if pid not in loaded_post_ids]\n",
    "\n",
    "    print(f\"Topic {topic_id}: Total posts in stream: {len(all_post_ids_in_stream)}, Initially loaded: {len(loaded_post_ids)}, Missing: {len(missing_post_ids)}\")\n",
    "\n",
    "    if not missing_post_ids:\n",
    "        print(f\"All posts for topic {topic_id} already loaded in initial fetch.\")\n",
    "        return topic_data\n",
    "\n",
    "    fetched_additional_posts = []\n",
    "    for i in range(0, len(missing_post_ids), POST_ID_BATCH_SIZE):\n",
    "        batch_ids = missing_post_ids[i:i + POST_ID_BATCH_SIZE]\n",
    "\n",
    "        query_params = [(\"post_ids[]\", pid) for pid in batch_ids]\n",
    "        posts_url = urljoin(base_url, f\"t/{topic_id}/posts.json\")\n",
    "\n",
    "        print(f\"Fetching batch of {len(batch_ids)} posts for topic {topic_id} (IDs: {batch_ids[0]}...{batch_ids[-1]})\")\n",
    "\n",
    "        try:\n",
    "            batch_response = requests.get(posts_url, params=query_params, cookies=cookies, timeout=60)\n",
    "            batch_response.raise_for_status()\n",
    "            batch_data = batch_response.json()\n",
    "\n",
    "            if isinstance(batch_data, list):\n",
    "                 fetched_additional_posts.extend(batch_data)\n",
    "            elif \"post_stream\" in batch_data and \"posts\" in batch_data[\"post_stream\"]:\n",
    "                fetched_additional_posts.extend(batch_data[\"post_stream\"][\"posts\"])\n",
    "            elif \"posts\" in batch_data and isinstance(batch_data[\"posts\"], list):\n",
    "                 fetched_additional_posts.extend(batch_data[\"posts\"])\n",
    "            else:\n",
    "                print(f\"Warning: Unexpected JSON structure for post batch in topic {topic_id}. Data: {str(batch_data)[:200]}...\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch post batch for topic {topic_id} (IDs: {batch_ids}): {e}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON for post batch in topic {topic_id}. Response: {batch_response.text[:200]}...\")\n",
    "\n",
    "    if fetched_additional_posts:\n",
    "        print(f\"Successfully fetched {len(fetched_additional_posts)} additional posts for topic {topic_id}.\")\n",
    "        existing_posts_in_topic_data = {post['id']: post for post in topic_data[\"post_stream\"][\"posts\"]}\n",
    "        for post in fetched_additional_posts:\n",
    "            if post['id'] not in existing_posts_in_topic_data:\n",
    "                topic_data[\"post_stream\"][\"posts\"].append(post)\n",
    "                existing_posts_in_topic_data[post['id']] = post\n",
    "\n",
    "        post_id_to_post_map = {post['id']: post for post in topic_data[\"post_stream\"][\"posts\"]}\n",
    "\n",
    "        sorted_posts = []\n",
    "        for post_id_val in all_post_ids_in_stream: # Renamed post_id to post_id_val to avoid conflict\n",
    "            if post_id_val in post_id_to_post_map:\n",
    "                sorted_posts.append(post_id_to_post_map[post_id_val])\n",
    "\n",
    "        topic_data[\"post_stream\"][\"posts\"] = sorted_posts\n",
    "        print(f\"Topic {topic_id}: Final post count in JSON: {len(topic_data['post_stream']['posts'])}\")\n",
    "\n",
    "    return topic_data\n",
    "\n",
    "\n",
    "def save_topic_json(topic_id, json_data, output_dir):\n",
    "    \"\"\"Saves the topic JSON data to a file.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filepath = os.path.join(output_dir, f\"topic_{topic_id}.json\")\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        # print(f\"Successfully saved topic {topic_id} to {filepath}\") # Reduced verbosity\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving topic {topic_id} to {filepath}: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the downloading process.\"\"\"\n",
    "    print(\"Script started.\")\n",
    "    cookies = parse_cookie_string(RAW_COOKIE_STRING)\n",
    "    if not cookies and DISCOURSE_BASE_URL != \"https://meta.discourse.org/\":\n",
    "        print(\"Warning: Running without cookies. This may fail for private forums or specific content.\")\n",
    "\n",
    "    topic_ids = get_topic_ids(\n",
    "        DISCOURSE_BASE_URL,\n",
    "        CATEGORY_SLUG,\n",
    "        CATEGORY_ID,\n",
    "        START_DATE,\n",
    "        END_DATE,\n",
    "        cookies\n",
    "    )\n",
    "\n",
    "    if not topic_ids:\n",
    "        print(\"No topic IDs found for the given criteria. Exiting.\")\n",
    "        return\n",
    "\n",
    "    total_topics = len(topic_ids)\n",
    "    success_downloads = 0\n",
    "    failed_topic_ids = []\n",
    "\n",
    "    print(f\"\\nStarting download of {total_topics} topics...\\n\")\n",
    "\n",
    "    for i, topic_id in enumerate(topic_ids, 1):\n",
    "        print(f\"--- [{i}/{total_topics}] Processing topic ID: {topic_id} ---\")\n",
    "        topic_json_data = get_full_topic_json(DISCOURSE_BASE_URL, topic_id, cookies)\n",
    "        if topic_json_data:\n",
    "            save_topic_json(topic_id, topic_json_data, OUTPUT_DIR)\n",
    "            success_downloads += 1\n",
    "        else:\n",
    "            print(f\"Failed to get complete data for topic {topic_id}.\")\n",
    "            failed_topic_ids.append(topic_id)\n",
    "        # print(f\"--- Finished processing topic ID: {topic_id} ---\\n\") # Reduced verbosity\n",
    "\n",
    "    print(\"\\n========= SUMMARY =========\")\n",
    "    print(f\"Total topics identified: {total_topics}\")\n",
    "    print(f\"Successfully downloaded full data for: {success_downloads} topics\")\n",
    "    print(f\"Failed to download/process: {len(failed_topic_ids)} topics\")\n",
    "    if failed_topic_ids:\n",
    "        print(\"Failed topic IDs:\", failed_topic_ids)\n",
    "    print(f\"Downloaded files are in: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "    print(\"Script finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5956a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1001 Document objects\n",
      "Saved normalized_docs.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "def load_and_normalize():\n",
    "    docs = []\n",
    "    # 1. Course‐content chunks\n",
    "    with open(\"tds_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        chunks = json.load(f)\n",
    "    for c in chunks:\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=c[\"chunk_text\"],\n",
    "                metadata={\n",
    "                    \"source_url\": c[\"source_url\"],\n",
    "                    \"title\":      c[\"title\"],\n",
    "                    \"chunk_index\":c[\"chunk_index\"],\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 2. Discourse‐content chunks\n",
    "    with open(\"tds_discourse_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        posts = json.load(f)\n",
    "    for p in posts:\n",
    "        # raw content lives in `content`\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=p[\"content\"],\n",
    "                metadata={\n",
    "                    \"topic_id\":    p[\"topic_id\"],\n",
    "                    \"topic_title\": p[\"topic_title\"],\n",
    "                    \"post_id\":     p[\"post_id\"],\n",
    "                    \"created_at\":  p[\"created_at\"],\n",
    "                    \"url\":         p[\"url\"],\n",
    "                    \"chunk_index\": p.get(\"chunk_index\", 0),\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return docs\n",
    "\n",
    "docs = load_and_normalize()\n",
    "print(f\"Loaded {len(docs)} Document objects\")\n",
    "\n",
    "# Convert to JSON-serializable dict format\n",
    "normalized_docs = [\n",
    "    {\n",
    "        \"page_content\": doc.page_content,\n",
    "        \"metadata\": doc.metadata\n",
    "    }\n",
    "    for doc in docs\n",
    "]\n",
    "\n",
    "# Save to disk\n",
    "with open(\"normalized_docs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(normalized_docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved normalized_docs.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32289f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 243 chunks to C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\\tds_discourse_threads.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "from bs4 import BeautifulSoup  # pip install beautifulsoup4\n",
    "\n",
    "# Regex patterns\n",
    "MD_IMAGE_RE = re.compile(r'!\\[[^\\]]*\\]\\((https?://[^\\)]+)\\)')\n",
    "MD_LINK_RE  = re.compile(r'\\[[^\\]]*\\]\\((https?://[^\\)]+)\\)')\n",
    "URL_RE      = re.compile(r'(https?://[^\\s)]+)')\n",
    "\n",
    "def clean_text(text: str) -> tuple[str, list[str]]:\n",
    "    \"\"\"\n",
    "    1. Pull out all URLs from markdown images, links, and bare URLs.\n",
    "    2. Remove them from the text.\n",
    "    3. Strip HTML tags, inline code ticks, headings, bullets, and garbage lines.\n",
    "    4. Return (cleaned_text, unique_links_list).\n",
    "    \"\"\"\n",
    "    links: list[str] = []\n",
    "\n",
    "    # 1. Extract markdown image URLs\n",
    "    text = MD_IMAGE_RE.sub(lambda m: links.append(m.group(1)) or '', text)\n",
    "    # 2. Extract markdown link URLs\n",
    "    text = MD_LINK_RE.sub(lambda m: links.append(m.group(1)) or '', text)\n",
    "    # 3. Extract bare URLs\n",
    "    text = URL_RE.sub(lambda m: links.append(m.group(1)) or '', text)\n",
    "\n",
    "    # 4. Strip any remaining HTML\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # 5. Remove inline code fences/backticks\n",
    "    text = re.sub(r'`([^`]*)`', r'\\1', text)\n",
    "\n",
    "    # 6. Clean line-by-line\n",
    "    cleaned_lines = []\n",
    "    for line in text.splitlines():\n",
    "        # trim whitespace\n",
    "        line = line.strip()\n",
    "        # drop headings\n",
    "        line = re.sub(r'^#+\\s*', '', line)\n",
    "        # drop bullets/quotes\n",
    "        line = re.sub(r'^[\\-\\*\\>\\s]+', '', line)\n",
    "        # skip very short or non-alphanumeric lines\n",
    "        if len(line) < 3 or not re.search(r'\\w', line):\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    # 7. Re-join & collapse multiple blanks\n",
    "    cleaned = \"\\n\".join(cleaned_lines)\n",
    "    cleaned = re.sub(r'\\n{2,}', \"\\n\\n\", cleaned)\n",
    "\n",
    "    # 8. Deduplicate links (preserve order)\n",
    "    seen = set()\n",
    "    unique_links = []\n",
    "    for url in links:\n",
    "        if url not in seen:\n",
    "            seen.add(url)\n",
    "            unique_links.append(url)\n",
    "\n",
    "    return cleaned, unique_links\n",
    "\n",
    "def load_and_normalize(base_path: str) -> list[Document]:\n",
    "    docs: list[Document] = []\n",
    "\n",
    "    # Course-content chunks\n",
    "    with open(os.path.join(base_path, \"tds_chunks.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        course_chunks = json.load(f)\n",
    "    for c in course_chunks:\n",
    "        clean, links = clean_text(c[\"chunk_text\"])\n",
    "        meta = {\n",
    "            \"source_url\":  c[\"source_url\"],\n",
    "            \"title\":       c[\"title\"],\n",
    "            \"chunk_index\": c[\"chunk_index\"],\n",
    "            \"links\":       links,\n",
    "        }\n",
    "        docs.append(Document(page_content=clean, metadata=meta))\n",
    "\n",
    "    # Discourse-content chunks\n",
    "    with open(os.path.join(base_path, \"tds_discourse_chunks.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        forum_posts = json.load(f)\n",
    "    for p in forum_posts:\n",
    "        raw = p.get(\"content\", \"\")\n",
    "        clean, links = clean_text(raw)\n",
    "        meta = {\n",
    "            \"topic_id\":    p[\"topic_id\"],\n",
    "            \"topic_title\": p[\"topic_title\"],\n",
    "            \"post_id\":     p[\"post_id\"],\n",
    "            \"created_at\":  p[\"created_at\"],\n",
    "            \"url\":         p[\"url\"],\n",
    "            \"chunk_index\": p.get(\"chunk_index\", 0),\n",
    "            \"links\":       links,\n",
    "        }\n",
    "        docs.append(Document(page_content=clean, metadata=meta))\n",
    "\n",
    "    return docs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BASE = r\"C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\"\n",
    "    docs = load_and_normalize(BASE)\n",
    "    print(f\"Loaded & cleaned {len(docs)} documents.\")\n",
    "\n",
    "    # Write out a simple JSON list of dicts\n",
    "    normalized = [\n",
    "        {\"page_content\": d.page_content, \"metadata\": d.metadata}\n",
    "        for d in docs\n",
    "    ]\n",
    "    with open(os.path.join(BASE, \"normalized_docs.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(normalized, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Saved cleaned normalized_docs.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f67287ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 424 normalized chunks to C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\\normalized_docs2.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import tiktoken            # pip install tiktoken\n",
    "from bs4 import BeautifulSoup  # pip install beautifulsoup4\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIG\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "BASE_DIR    = r\"C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\"  # adjust as needed\n",
    "IN_COURSE   = os.path.join(BASE_DIR, \"tds_chunks.json\")\n",
    "IN_FORUM    = os.path.join(BASE_DIR, \"tds_discourse_chunks.json\")\n",
    "OUT_NORMAL  = os.path.join(BASE_DIR, \"normalized_docs2.json\")\n",
    "\n",
    "MAX_TOKENS  = 500\n",
    "ENC_MODEL   = \"gpt-3.5-turbo\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# PATTERNS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "MD_IMAGE_RE = re.compile(r'!\\[[^\\]]*\\]\\((https?://[^\\)]+)\\)')\n",
    "MD_LINK_RE  = re.compile(r'\\[[^\\]]*\\]\\((https?://[^\\)]+)\\)')\n",
    "URL_RE      = re.compile(r'(https://[^\\s)]+)')\n",
    "AT_RE       = re.compile(r'@[\\w\\-]+')\n",
    "MULTI_SPACE = re.compile(r' {2,}')\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CLEAN & EXTRACT LINKS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def clean_and_extract(text: str) -> (str, list[str]):\n",
    "    links = []\n",
    "    # extract and remove markdown images\n",
    "    text = MD_IMAGE_RE.sub(lambda m: links.append(m.group(1)) or '', text)\n",
    "    # extract and remove markdown links\n",
    "    text = MD_LINK_RE.sub(lambda m: links.append(m.group(1)) or '', text)\n",
    "    # extract and remove bare URLs\n",
    "    text = URL_RE.sub(lambda m: links.append(m.group(1)) or '', text)\n",
    "    # remove @mentions\n",
    "    text = AT_RE.sub('', text)\n",
    "    # strip HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    # remove inline code ticks\n",
    "    text = re.sub(r'`([^`]*)`', r'\\1', text)\n",
    "    # collapse multi-space to single\n",
    "    text = MULTI_SPACE.sub(' ', text)\n",
    "    # split lines and clean\n",
    "    lines = []\n",
    "    for L in text.splitlines():\n",
    "        L = L.strip()\n",
    "        # drop empty or too short lines\n",
    "        if len(L) < 4:\n",
    "            continue\n",
    "        # drop markdown headings/bullets\n",
    "        L = re.sub(r'^[#>\\-\\*\\s]+', '', L)\n",
    "        # collapse within-line multi-spaces again\n",
    "        L = MULTI_SPACE.sub(' ', L)\n",
    "        if L:\n",
    "            lines.append(L)\n",
    "    cleaned = \"\\n\".join(lines)\n",
    "    # dedupe links\n",
    "    seen = set(); uniq = []\n",
    "    for u in links:\n",
    "        if u not in seen:\n",
    "            seen.add(u); uniq.append(u)\n",
    "    return cleaned, uniq\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CHUNKING\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "enc = tiktoken.encoding_for_model(ENC_MODEL)\n",
    "def chunk(text: str) -> list[str]:\n",
    "    ids = enc.encode(text)\n",
    "    return [enc.decode(ids[i:i+MAX_TOKENS]) for i in range(0, len(ids), MAX_TOKENS)]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# MAIN NORMALIZATION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    out = []\n",
    "    # process course chunks\n",
    "    for c in json.load(open(IN_COURSE, \"r\", encoding=\"utf-8\")):\n",
    "        txt, links = clean_and_extract(c[\"chunk_text\"])\n",
    "        md = {\n",
    "            \"source_url\": c[\"source_url\"],\n",
    "            \"title\":      c[\"title\"],\n",
    "            \"chunks\":     len(chunk(txt)),\n",
    "            \"links\":      links\n",
    "        }\n",
    "        for idx, ch in enumerate(chunk(txt)):\n",
    "            out.append({\n",
    "                \"type\":         \"course\",\n",
    "                \"chunk_index\":  idx,\n",
    "                \"page_content\": ch,\n",
    "                \"metadata\":     md\n",
    "            })\n",
    "    # group forum posts by topic\n",
    "    threads = defaultdict(list)\n",
    "    for p in json.load(open(IN_FORUM, \"r\", encoding=\"utf-8\")):\n",
    "        threads[p[\"topic_id\"]].append(p)\n",
    "    # merge and chunk each thread\n",
    "    for tid, posts in threads.items():\n",
    "        posts.sort(key=lambda x: x[\"post_number\"])\n",
    "        parts = []\n",
    "        for p in posts:\n",
    "            tag = \"Q\" if p[\"post_number\"] == 1 else f\"R{p['post_number']-1}\"\n",
    "            parts.append(f\"{tag}: {p.get('content','')}\")\n",
    "        full = \"\\n\\n\".join(parts)\n",
    "        txt, links = clean_and_extract(full)\n",
    "        md = {\n",
    "            \"topic_id\":    tid,\n",
    "            \"topic_title\": posts[0][\"topic_title\"],\n",
    "            \"url\":         posts[0][\"url\"],\n",
    "            \"chunks\":      len(chunk(txt)),\n",
    "            \"links\":       links\n",
    "        }\n",
    "        for idx, ch in enumerate(chunk(txt)):\n",
    "            out.append({\n",
    "                \"type\":         \"forum\",\n",
    "                \"chunk_index\":  idx,\n",
    "                \"page_content\": ch,\n",
    "                \"metadata\":     md\n",
    "            })\n",
    "    # save normalized\n",
    "    with open(OUT_NORMAL, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(out)} normalized chunks to {OUT_NORMAL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc20512",
   "metadata": {},
   "source": [
    "# embedding testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b19015f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 123617\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(r\"C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\\normalized_docs2.json\", encoding=\"utf-8\") as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "total_chars = sum(len(doc[\"page_content\"]) for doc in docs)\n",
    "avg_chars_per_token = 4  # Approximate average\n",
    "total_tokens = total_chars // avg_chars_per_token\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed520119",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'genai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mClient(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39membed_content(\n\u001b[0;32m      6\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-embedding-exp-03-07\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m         contents\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the meaning of life?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'genai'"
     ]
    }
   ],
   "source": [
    "import genai\n",
    "\n",
    "client = genai.Client(api_key=\"GEMINI_API_KEY\")\n",
    "\n",
    "result = client.models.embed_content(\n",
    "        model=\"gemini-embedding-exp-03-07\",\n",
    "        contents=\"What is the meaning of life?\")\n",
    "\n",
    "print(result.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77ea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elara wasn't your typical twelve-year-old. While others obsessed over pop stars and social media, Elara craved adventure.  Her escape was the dusty attic of her grandmother's rambling Victorian house, a place filled with forgotten treasures and the scent of mothballs and old paper.  It was there, nestled amongst chipped teacups and faded photographs, that she found it – a worn leather backpack, its stitching intricate and oddly shimmering.\n",
      "\n",
      "The backpack was unremarkable except for a small, tarnished silver clasp shaped like a hummingbird.  Curiosity piqued, Elara opened the clasp.  Instead of the expected emptiness, she found it brimming with… well, everything.  A perfectly ripe mango, a first edition copy of \"Alice's Adventures in Wonderland,\" a tiny, singing frog, a map drawn on parchment that seemed to glow faintly, and a compass that spun wildly, pointing in all directions at once.\n",
      "\n",
      "Over the next few days, Elara’s life transformed.  The backpack, she discovered, was magical.  It provided whatever she needed, as long as she truly desired it.  A torrential downpour? The backpack produced a waterproof cloak.  A difficult math problem?  A tiny tutor, no bigger than her thumb, materialized, explaining equations with surprising clarity.  The mango, once consumed, was replaced by another, seemingly inexhaustible.\n",
      "\n",
      "Her adventures, however, were not without peril.  The compass, she learned, pointed not to geographical locations but to places of emotional significance.  One day, it led her to a hidden grove where she found her estranged grandfather, a man she only knew through faded photographs. Their reunion, though bittersweet, was a powerful experience.\n",
      "\n",
      "The map, however, held a different kind of challenge. It depicted a path winding through a fantastical landscape, leading to a legendary waterfall said to grant wishes.  Elara, fueled by a yearning for a deeper connection with her family and a thirst for adventure, decided to follow it.\n",
      "\n",
      "The backpack, of course, provided the necessary tools: climbing gear that seemed to magically adjust to any terrain, sturdy boots that never wore down, and a supply of incredibly delicious, magically self-replenishing sandwiches.  Her journey was fraught with bizarre creatures – singing mushrooms, mischievous pixies, and a grumpy, one-eyed griffin who guarded a particularly treacherous bridge.  Each challenge tested her courage and ingenuity, forcing her to rely on her resourcefulness as much as the backpack’s magic.\n",
      "\n",
      "Finally, she reached the waterfall.  The shimmering cascade was breathtaking, and as she stood before it, she didn't wish for wealth or power. Instead, she wished for her family to be whole, for the connection she craved.  A warm light enveloped her, and when she opened her eyes, she found herself back in the attic, the backpack resting quietly beside her.  But something had changed. The clasp, once tarnished, now gleamed brilliantly, and the faint scent of wildflowers lingered in the air, a subtle reminder of her extraordinary adventure. The magical backpack was still there, filled with the promise of more adventures to come, but now Elara knew the greatest magic wasn’t in the backpack itself, but in the courage and kindness she found within her own heart.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "prompt = \"Write a story about a magic backpack.\"\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "print(response.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c230e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: [0.053849667, -0.040725958, -0.0046300436, -0.03277522, 0.015930427, 0.026341947, 0.03961317, -0.015965927, -0.007771914, 0.038305927, 0.053404994, -0.01101266, -0.0064206608, -0.0338029, -0.017255515, 0.0022145999, -0.0009908334, -0.008556645, 0.008218227, -0.022017727, 0.0023225478, 0.0049427766, -0.0041448046, -0.0005308099, 0.0027797983, -0.024324711, 0.04580033, -0.03871397, -0.019353203, 0.015879262, -0.061900564, 0.050104205, -0.067114435, -0.013838072, 0.008133515, -0.045278054, -0.02982182, -0.009732122, 0.026702441, 0.022631288, -0.0059094853, -0.009791319, 0.008383838, -0.0486036, 0.047498666, -0.01581222, -0.0112596, 0.037022218, -0.0045725727, -0.056783926, 0.054819044, -0.020024724, 0.03964844, -0.008953915, -0.012323955, -0.038888805, 0.057950422, 0.008822417, -0.049521614, -0.008155741, -0.0062932796, -0.032998957, -0.025061931, 0.062351584, -0.03447409, -0.07040939, -0.051851302, 0.01860223, 0.04383988, -0.041436013, 0.010627225, -0.027357712, 0.050579593, 0.007494121, -0.0624475, -0.07308838, -0.015102245, 0.072976224, 0.044871762, 0.019046584, 0.0076298784, -0.0061946106, -0.031929832, -0.045971707, -0.071690075, 0.046995718, -0.038584195, -0.002486957, -0.024028, 0.09575981, 0.036465593, 0.006070247, -0.017046876, -0.090275824, 0.014087881, 0.046859723, 0.006305165, 0.026866827, -0.034209546, 0.00096969184, -0.02224218, 0.0047459947, -0.017900193, 0.016149692, 0.03510809, -0.01266224, 0.04817376, 0.06700716, -0.029486287, 0.052232962, -0.050248448, -0.024900725, -0.015126254, -0.03272669, 0.020637974, -0.042964417, -0.0015591199, 0.048050854, 0.041585866, -0.027144833, 0.014856254, 0.008163232, 0.043338276, -0.025094362, 0.012793818, 0.05720424, 0.028538067, 0.027433652, 0.06721369, 0.046589755, 0.007968342, -0.01933451, 0.0527608, 0.010776582, 0.0632892, 0.028658105, 0.03228899, 0.03273676, 0.01849364, -0.011847168, 0.0134158535, 0.0030386199, -0.011511102, 0.07416557, -0.010862218, -0.0153510915, -0.016987089, -0.02117089, -0.0048529087, -0.031140046, -0.019911472, -0.021171326, -0.11650669, 0.02915389, 0.066504195, -0.027268576, -0.009419186, 0.031214772, -0.008601043, -0.009558892, 0.051230956, -0.0121161435, 0.010589046, 0.005825502, 0.031183792, 0.008411998, 0.0121267745, -0.003060327, -0.03737611, -0.02535414, -0.021228656, 0.014055195, -0.05349608, -0.053851705, -0.037196226, -0.040714387, -0.030610934, -0.015728002, -0.009543579, -0.0077859894, -0.004647887, -0.05654648, -0.015299786, 0.010052081, 0.058495656, -0.018235812, 0.058482002, -0.056969915, -0.035267282, -0.014183847, -0.031796843, 0.00566222, -0.028346814, -0.05947726, -0.016634012, 0.0131773595, -0.002956299, 0.029073855, 0.029720834, -0.012727553, -0.003039579, 0.09563733, 0.026667234, 0.009968245, 0.00811694, -0.013820402, 0.091162324, -0.05679705, -0.018288659, 0.0004483707, -0.02291318, -0.020678528, -0.04190161, -0.0011391542, 0.010790473, 0.008105118, 0.03413429, 0.029287554, -0.010231988, -0.010352303, 0.00592637, -0.042258754, -0.032530148, 0.0080610225, 0.015519997, 0.03454574, -0.021583686, 0.038527, -0.009366924, -0.028831225, 0.04371864, 0.07091756, 0.04381269, -0.009471703, 0.037220262, -0.041939773, -0.025954237, 0.0074724294, 0.008627256, 0.008375643, -0.03878206, 0.026233565, 0.029468827, 0.013238136, -0.009980546, -0.017872456, 0.0077451994, 0.0216686, -0.017150065, 0.022959735, -0.026002778, -0.025826761, 0.030575149, 0.024441268, -0.04720201, 0.0469033, -0.030003533, -0.014232834, -0.018915761, 0.029293517, 0.07217559, 0.029083071, 3.1327265e-05, -0.027400348, -0.023829479, -0.013158578, 0.04279352, -0.07519377, -0.028601749, 0.056091323, -0.015596716, -0.06610377, 0.030684434, 0.008006954, -0.03787884, 0.053185314, -0.007643786, 0.02842259, 0.05196142, -0.0122189, 0.036486138, -0.0030878657, -0.0037252938, -0.037613355, 0.0011025113, 0.015269529, -0.020023158, -0.014498036, 0.049422126, -0.022385232, -0.051369306, -0.006908845, -0.03102226, -0.05357849, -0.026246022, -0.02085035, 0.011412389, 0.01308393, 0.021732943, -0.009000569, -0.018155498, -0.046887733, -0.0164488, -0.048339102, 0.028533489, 0.03639813, -0.0058614137, -0.043352433, 0.0070915823, 0.011650792, 0.0015546934, -0.006099596, -0.038239334, 0.016475845, 0.060393184, 0.029713599, -0.042707704, 0.013008637, -0.06378355, 0.07908019, 0.023577336, 0.051775377, 0.029762387, -0.018312812, 0.006247431, 0.009087751, -0.021266488, 0.0669058, -0.03376884, 0.018998263, -0.0033997602, -0.03949103, -0.008066569, 0.025764674, 0.015088145, 0.04214736, -0.07944771, -0.0152242305, 0.01672139, -0.035106983, 0.010723307, 0.032459248, -0.047733195, -0.07440905, 0.041496653, -0.022089494, 0.00986232, -0.016745646, 0.096094996, 0.0072916173, 0.023433957, 0.07978965, -0.030205728, 0.016875617, -0.05190192, -0.008001805, 0.070418656, -5.0207196e-05, 0.019911263, -0.02689367, -0.04767966, 0.04143817, -0.021620983, 0.01422031, 0.00070113357, 0.038193, -0.060675435, -0.011223576, -0.0006016674, -0.020217827, 0.025902575, -0.05043981, 0.007140089, -0.03913905, -0.03165175, -0.010625418, -0.005519879, -0.015131954, 0.04481206, -0.013599913, -0.004479265, -0.013564294, 0.055839848, 0.021463687, 0.012210182, -0.04834668, 0.054242894, -0.0049464307, 0.01474932, -0.0031349633, 0.010618011, 0.033932026, 0.06968166, 0.029582754, 0.035392143, 0.0012018905, -0.016757315, -0.035042454, 0.0448479, -0.01387447, -0.006195108, 0.009860618, -0.05547144, -0.01575868, -0.020965805, 0.009846461, -0.039319262, -0.02975083, 0.0028329927, -0.008329652, 0.035038058, 0.04896343, 0.026168449, -0.06694011, -0.02850533, -0.038529653, 0.047697574, -0.007641777, 0.020439588, 0.021307694, 0.0044347933, 0.002050557, 0.020334467, -0.0717087, -0.050162513, -0.015040663, -0.021968085, -0.051646944, 0.003678216, 0.025487667, 0.0066728303, 0.011601096, -0.0022069288, 0.013513207, -0.024950383, -0.034913242, 0.019198822, 0.020638864, 0.0039651343, -0.031025596, 0.031522796, -0.03387482, 0.027513282, 0.03515602, -0.05515729, -0.008665744, -0.01108513, -0.06847987, 0.05376041, -0.10480322, 0.004221568, -0.048994698, -0.02252354, -0.06882283, 0.015790498, -0.046904262, 0.008191099, 0.07194558, -0.013383144, 0.00028046715, -0.013871649, -0.021243596, -0.007194048, -0.116328046, -0.016601512, -0.053778417, 0.044499256, -0.033842683, 0.07251521, 0.060689036, -0.003624704, -0.008533887, 0.0014153925, 0.014015101, -0.025070047, 0.01073395, -0.0776121, -0.0072579784, -0.0027985822, -0.055252653, -0.0053500845, 0.010499583, 0.044348847, 0.009147006, -0.055155188, -0.048177592, 0.017492253, 0.012053783, 0.0001845729, 0.060603656, -0.015696097, -0.013118046, -0.019049989, -0.04011172, -0.013337041, 0.022224221, -0.0224145, 0.05274605, 0.046567883, 0.040728774, 0.018554123, -0.008385111, -0.009924931, -0.025514401, 0.060054604, -0.09601144, 0.017595692, 0.04196192, -0.0006577784, -0.038698915, -0.015115626, 0.0120282285, 0.0314849, -0.0038058648, 0.04877677, -0.023292987, -0.016707832, -0.040955752, -0.015777998, -0.023982069, 0.091529526, 0.0071458877, -0.062969394, 0.018365357, 0.0005894452, -0.071127854, -0.015082501, 0.071057245, -0.020174133, 0.01213611, -0.036603134, 0.06716065, -0.098403536, -0.015378776, -0.0019198175, -0.050996937, -0.025786793, 0.022548933, 0.04190347, -0.018117297, 0.039479643, -0.052524313, 0.018912634, 0.004064695, 0.011949725, 0.012262104, 0.066854104, -0.10169469, 0.041077767, -0.013460255, -0.028907966, -0.031228766, 0.021712545, -0.010182847, 0.034476057, -0.018665234, -0.036686104, 0.01425885, 0.021556593, 0.029492533, -0.036899175, -0.024924168, 0.03715009, -0.01826516, 0.024981113, 0.010194425, -0.04463911, 0.0065617184, 0.0973035, -0.019825125, -0.0017198668, 0.0100225955, 0.0049531814, 0.0194382, 0.06419426, 0.016363045, -0.012182039, -0.019005092, -0.032810546, -0.036178313, 0.03254047, 0.008827343, -0.02248397, 0.06487451, -0.022449575, 0.038652424, 0.03263605, -0.0018785336, 0.03155437, 0.058563516, -0.044019185, 0.03801428, -0.059959333, -0.035861865, 0.013579629, 0.0185441, 0.0021209617, -0.00087367836, -0.009456635, -0.019061144, 0.018464461, -0.06692296, 0.06144457, -0.024868082, -0.018582946, 0.029791981, -0.015940785, -0.023296831, -0.033791494, 0.025178753, -0.02682794, -0.016800378, 0.06062292, -0.016850313, -0.04032051, -0.013990651, 0.026173059, -0.004223659, -0.028975151, -0.023728445, -0.02419832, 0.00020666354, 0.019688535, 0.025038986, 0.079309344, -0.023625305, -0.0097109405, -0.015329244, 0.03639013, 0.022017138, 0.043217387, 0.042141844, -0.0045142695, 0.006062651, -0.017892607, 0.048434675, -0.033542916, -0.027570065, 0.05116265, 0.005315609, -0.088189594, 0.024867289, 0.046462674, -0.021953655, 0.021357775, 0.07034717, 0.03449925, -0.050980143, -0.012706538, 0.012203395, -0.008694261, -0.034135845, 0.05689512, -0.008932162, -0.040952697, 0.009486779, -0.06208646, -0.032085657, 0.018368443, -0.0065785, -0.021836981, 0.030514793, 0.0005585415, -0.008415668, 0.0024963468, -0.039737772, -0.045433104, -0.049868967, -0.06576063, 0.028064592, -0.06613624, 0.018006433, 0.033582218, -0.014203984, -0.0033718084, 0.0090879975, -0.059702516, 0.06478585, 0.0014968527, 0.041670788, -0.03470084, -0.0152728055, -0.049097095, 0.021775862, -0.039512362, 0.030909913, 0.043364514, -0.010157879, -0.01881611, -0.032953363, 0.0005119291, -0.015499219, -0.054605775, -0.002540493, 0.06810067, 0.010393994, -0.031777076, -0.013213481, 0.009588534, 0.06465014, 0.003188871, -0.03868388, -0.0023593723, 9.435056e-05, -0.01665339, -0.0022836353, -0.014198232, 0.048088226, -0.0020722763, 0.009228589, 0.049232587, -0.043981936, -0.031765897, 0.05277269, 0.042674236, -0.009774291, 0.040948976, -0.037202835, -0.03552201, 0.07203039, 0.021981973, 0.0009867083, -0.00039360355, -0.024408065, -0.02388425, 0.020306487, -0.035898272, 0.039020147, -0.04801355, -0.042875398, 0.07574854, -0.052901078, 0.039412964, 0.03230539, -0.086369425, 0.071143776, -0.00043535425, 0.027530884, 0.008095643, -0.036188927, -0.023480572, -0.027103586, 0.018112753, 0.07601486, -0.03438949, -0.041435204, 0.021406816, -0.0106890565, -0.0061223954, -0.012245736, -0.040127803, -0.031582244, 0.020612828, 0.03424249, 0.06282054, -0.033454, 0.0009761768, 0.016114663, 0.0068485714, 0.02352548, -0.07224652, 0.039801598, -0.055349514, 0.030059917, 0.043991193, 0.051011126, -0.028286707, 0.016645802]\n",
      "\n",
      "Multiple Embeddings: [[0.041507974, -0.05714845, 0.0130555825, -0.060511906, 0.0679444, 0.061069697, 0.014527526, 0.018093748, 0.03593546, 0.0055291555, 0.024882272, 0.025913555, 0.007717266, -0.017970888, -0.029778743, -0.045620594, 0.018164571, -0.0075673033, -0.01266809, -0.010761325, 0.008402478, 0.034824736, -0.015980104, 0.029636644, -0.02253802, 0.02211148, 0.045873553, -0.07790394, -0.010805919, 0.0630943, -0.037254244, 0.0024807206, -0.096055605, -0.01016462, 0.0062446552, -0.035190873, -0.0043590767, 0.035139404, 0.009273383, 0.030427804, -0.012495993, -0.004310018, -0.0391324, -0.0435359, 0.04359002, 0.00095621496, -0.009603183, -0.015323576, -0.007936483, -0.055322304, 0.061833132, -0.01749913, 0.07372878, -0.026471244, -0.014571045, -0.0025524637, 0.08389233, 0.018299626, -0.029902264, -0.0027256017, -0.004319831, 0.027050504, -0.023921946, 0.030971885, -0.010317443, -0.019641263, -0.042286936, 0.0031727545, 0.07373272, -0.025376989, 0.0071945614, -0.0658827, 0.039906673, -0.0153477285, -0.024053007, -0.10486573, -0.038837, 0.03834691, 0.050003935, 0.022990994, -0.023126211, -0.06005442, -0.032055084, -0.05592902, -0.031800628, 0.011339626, -0.048430234, 0.024410779, 0.01418357, 0.033567637, 0.02155245, 0.0018439991, -0.024903301, -0.06484817, -0.02222418, 0.006512381, -0.008886564, -0.009159259, -0.0070752893, -0.00016641564, -0.058627684, -0.050365712, 0.009375379, 0.018212577, 0.027622962, -0.0018720853, 0.016657012, -0.0018067063, 0.009224352, 0.028681247, -0.034269575, 0.010643235, -0.024563907, -0.04789751, 0.010609076, 0.00028050496, 0.0111921765, 0.061696555, 0.045241337, -0.011930013, 0.029168837, -0.029891957, 0.019649865, 0.029486893, 0.030784514, 0.029999701, -0.02022608, 0.051985454, 0.041655064, 0.007421715, 0.013788959, -0.01636868, 0.035685495, 0.03390512, 0.105553485, 0.047773298, 0.023150789, -0.029484427, 0.01728108, -0.025368595, -0.019200686, 0.026728433, -0.014399664, 0.048369348, -0.0062719416, 0.009473448, -0.034896757, -0.004245459, -0.017624792, -0.015620408, -0.022661636, -0.019070685, -0.0810753, 0.015843755, 0.090964414, 0.0020829167, -0.02585295, -0.00696527, 0.011118218, 0.0032731898, 0.018481256, 0.016369022, 0.058900077, 0.0072768363, -0.010025257, 0.010202656, -0.015100164, -0.0057502463, -0.018166013, -0.018403543, 0.0007344039, -0.0012456569, -0.055718657, -0.010421273, -0.00854578, -0.052803375, -0.026821358, -0.021723937, -0.0033125828, -0.025061151, 0.0005193293, -0.022673925, 0.02874961, -0.007642866, 0.03183066, 0.015779048, 0.05905008, -0.05373628, -0.05632839, -0.036014706, -0.023089036, 0.018630031, -0.019048287, -0.031439953, -0.0064653424, 0.026300874, -0.015385651, 0.006571, 0.054240692, -0.04195094, -0.016039982, 0.08760333, 0.015385612, 0.013666114, 0.03275571, 0.015088808, 0.05706534, -0.06092486, -0.03044329, -0.00042784162, -0.029253313, -0.030640893, -0.023307784, -0.02919573, 0.025819661, 0.015830543, 0.03047178, 0.0423189, 0.004011877, -0.023388242, -0.04869691, 0.0010762282, -0.010842185, 0.02338726, -0.012369452, 0.023972485, 0.0049432823, 0.038754836, 0.018023547, -0.041252796, 0.009995214, 0.0869293, 0.00445852, 0.050594326, 0.045073006, -0.03166031, 0.0047807726, -0.010825815, 0.008034856, 0.046528995, -0.05549192, 0.020114578, 0.04060832, 0.030772457, -0.058816776, -0.024168726, -0.029862389, 0.009935914, -0.01422848, 0.041392818, 0.010799134, -0.013856624, 0.007352328, 0.0037086408, -0.039407913, 0.043179464, -0.041708387, 0.028750913, -0.017667266, 0.012041148, 0.06055783, 0.0065044663, -0.0010105439, 0.0052228686, -0.015797464, 0.012717238, 0.061376315, -0.047287956, -0.015616558, 0.048278414, -0.010489127, -0.015876418, 0.038627077, -0.003803064, -0.007834784, 0.009879749, -0.0011705682, 0.027198354, 0.0051599983, -0.007052351, -0.010235238, 0.034022447, -0.024761861, -0.042078614, -0.058964558, -0.010543141, -0.047651935, -0.0046918667, 0.05663875, -0.05693266, -0.083917014, -0.017522972, 0.009499368, -0.030477956, -0.04326651, 0.027909374, -0.032286413, 0.030879077, 0.047122438, 0.01633343, -0.032970723, -0.047927853, 0.031987075, -0.040574573, 0.029685907, 0.010634784, 0.0072655724, -0.037459303, 0.01873658, 0.0016373859, -0.0022985947, -0.011722709, -0.050283574, -0.0058923783, 0.060218547, 0.030604249, -0.033222884, 0.03584507, -0.05497225, 0.030277498, 0.06945231, 0.069435515, 0.01627038, -0.020570032, -0.0019988196, 0.021047425, 0.026007177, 0.092400596, 0.0039898474, 0.005488933, -0.0246485, -0.015049326, -0.028483257, 0.047563944, -0.0044140457, -0.0040981537, -0.040828094, 0.01894849, 0.02019178, -0.0035188415, 0.01133858, 0.029865202, -0.0209585, -0.040069178, 0.03822979, 0.011486981, 0.009712126, -0.026586713, 0.07602996, 0.026593588, 0.027792333, 0.044426106, -0.07689872, -0.03064061, 0.019949596, -0.0531982, 0.08493992, 0.015725328, 0.04158657, -0.022775799, -0.033871964, 0.07900485, -0.03666097, 0.0069286926, 0.027829666, 0.05447678, -0.044447456, 0.0017863711, -0.048869427, 0.010300504, -0.023607742, -0.057477128, -0.010081105, -0.052042797, -0.024620334, 0.0075675575, -0.036020603, -0.0352693, -0.005628335, -0.03762608, -0.0150796445, -0.0103959385, 0.07190441, 0.04647109, -0.0032616109, -0.027725084, 0.007892634, 0.03734848, 0.008927902, 0.020255582, 0.02291987, 0.038565006, 0.05748763, 0.016710553, 0.029524956, -0.006437558, -0.022588499, -0.0569354, 0.04507958, -0.013544968, 0.00878776, -0.0007178592, -0.049281932, -0.012662438, -0.024894405, -0.012877374, -0.013809841, -0.04353521, -0.00029599192, -0.004157285, 0.05240674, 0.025813637, 0.024552321, -0.028059531, -0.06879521, -0.015606928, 0.051196117, -0.027089886, -0.018721031, 0.024220832, -0.011460999, 0.014362424, 0.005889927, -0.01826608, 0.010075694, -0.0070188125, -0.0026843345, -0.012224861, -0.022410389, 0.03238904, 0.044425655, 0.0033933565, -0.019995522, 0.0066861496, -0.057337634, -0.04262542, 0.02554442, 0.028163612, -0.013765853, -0.01548369, 0.049332503, -0.07440954, 0.028018607, -0.008015023, -0.03465376, 0.00899166, -0.0129435435, -0.07022352, 0.03695129, -0.074796945, -0.0040607187, -0.07181649, -0.008844139, -0.053869866, -0.008881748, -0.04949409, -0.015208649, 0.038589984, -0.0105401175, 0.033699475, -0.037447248, 0.003548579, -0.00060885126, -0.055976488, 0.028224682, -0.07305957, -0.019633003, -0.01925488, 0.051762607, 0.037850905, -0.02175152, -0.047088314, -0.00043160986, 0.0080020595, -0.044216156, 0.02462444, -0.06574585, -0.023379967, 0.019472383, -0.019356836, -0.03351614, 0.032479413, 0.030002171, 0.011250693, -0.017898241, 0.02459855, 0.0052897725, -0.00022328355, -0.02304477, 0.040916983, -0.034288853, -0.019467242, -0.0504995, -0.033820987, 0.020368693, -0.003059881, -0.0031759995, 0.050642606, 0.050375972, 0.07806015, -0.0048529846, -0.0004536568, -0.018827366, 0.016884094, 0.06706343, -0.065964796, 0.07156472, 0.038504336, -0.00014914757, -0.0029745705, -0.021656534, 0.025163678, 0.017792925, 0.008937699, 0.034251835, -0.048990227, -0.01966021, -0.010939208, 0.038812358, -0.04491953, 0.075928636, -0.015914002, -0.10080392, 0.048859164, -0.016785758, -0.063139066, -0.046446446, 0.021721827, -0.011649205, 0.028075166, -0.06777556, 0.08418698, -0.07810887, -0.0047108983, 0.00096431974, -0.05335553, 0.028650928, -0.01783667, 0.023225667, -0.009315582, 0.040417895, -0.048106007, 0.04158414, 0.008695013, -0.04236307, 0.03265962, 0.0049949335, -0.09322413, -0.007204343, -0.040426586, -0.008352316, -0.019047804, 0.030953562, -0.0357408, 0.052631613, -0.014712676, 0.0095195025, -0.0023996262, 0.02989236, 0.042804353, 0.005424565, -0.020163054, -0.023137864, -0.056742538, 0.087487556, -0.0030843702, -0.034030974, 0.018843623, 0.06264096, -0.018407783, 0.02004184, 0.029412743, 0.037187994, -0.045248285, 0.070222385, 0.022750163, -0.031487532, -0.008151999, 0.036011938, -0.0006386096, 0.028659323, -0.0025094906, -0.0063293637, 0.07120464, -0.040700503, 0.061988086, -0.0066325064, 0.012147806, 0.02803532, 0.0025618763, -0.027490435, 0.03278933, -0.107670054, 0.021586975, -0.03592558, -0.0043020663, 0.026023686, -0.011646708, -0.0049673687, -0.051434536, 0.05815996, -0.08406958, 0.04992991, -0.0433964, 0.012766316, -0.0054622935, -0.030985177, -0.0014133446, -0.01955375, 0.024143837, -0.028711054, -0.018776862, -0.013199117, -0.02164056, -0.011771733, -0.002884637, 0.031102203, 0.060057893, -0.043799672, -0.002094934, -0.02689602, 0.012648562, 0.002534174, 0.041731503, 0.02165581, -0.012042989, -0.0050381077, -0.051654432, 0.05427783, -0.012176002, 0.053278536, 0.08186657, -0.020027125, -0.010415811, -0.0283357, 0.062866814, 0.0114496555, -0.039428752, 0.039230358, 0.016518451, -0.090923354, -0.0066600232, 0.045965176, -0.012715654, 0.0010685653, 0.09354266, 0.014660195, -0.07580075, -0.0032352381, 0.043216184, -0.029496094, 0.0023110956, -0.008627083, -0.012696239, -0.02328711, -0.0009450966, -0.05068871, 0.0014104402, 0.0041284123, -0.04874777, -0.002967392, -0.005082213, 0.005426737, 0.00025637442, 0.006222634, 0.015281304, -0.047701303, -0.0638223, -0.028256554, 0.056562606, -0.06392182, 0.014031, 0.027609894, 0.029192956, 0.050357632, 0.021467444, -0.0025618616, 0.031328335, 0.0043457807, 0.03781524, -0.0025074317, -0.030004505, -0.045329653, 0.026527392, -0.00316276, -0.0036583808, 0.04408577, -0.04435888, -0.021376045, -0.041428003, -0.00090600626, 0.00085275515, -0.0017114382, -0.031371173, 0.04975305, -0.01514441, 0.011960943, -0.047932986, -0.022135818, 0.08143415, -0.0045098923, -0.023880307, -0.013001581, 0.0014631669, -0.023083583, 0.0385095, -0.019075748, 0.020475497, -0.03374729, 0.0024719962, 0.026881056, -0.023117281, -0.035282437, 0.026709028, 0.015743554, -0.054706387, 0.013143421, 0.0013670027, -0.033748623, 0.06437775, -0.002337933, 0.029313102, 0.044062674, -0.057644565, -0.054429032, 0.02548055, -0.040250067, 0.007282272, -0.019235874, -0.043798268, 0.06381425, -0.068121776, 0.029704213, -0.0008101586, -0.059074704, 0.073134094, -0.01568313, 0.024339728, -0.005297373, -0.06890916, -0.07520374, 0.031815488, 0.007801604, 0.09310898, 0.02194504, -0.07470677, 0.030828936, 0.004534238, -0.057679724, 0.003119957, -0.025915585, 0.003975146, 0.04608352, 0.031508546, 0.0052064005, -0.026488796, 0.0008688687, 0.017331464, -0.012781997, 0.016483555, -0.086184464, 0.031295035, -0.011855939, 0.031931374, 0.011031967, 0.04811195, -0.03671598, 0.004347111], [0.01326615, -0.05164329, -0.0011288009, -0.041525766, 0.054959934, 0.028358314, -0.005281423, -0.0055091083, 0.022703521, 0.027738443, -0.021171926, -0.012936297, 0.003579605, 0.0013883442, -0.04296295, -0.027334755, 0.04279529, 0.015706107, 0.0026617323, -0.002960105, 0.019077908, 0.018105475, -0.006193117, 0.024365056, -0.01110906, -0.0014320347, 0.054963004, -0.064203456, -0.028605824, 0.062342454, -0.041497793, -0.008467373, -0.10623793, -0.023901029, -0.01157708, -0.018076112, -0.012888959, 0.039120227, 0.020996004, 0.035971504, 0.002127904, -0.015126915, -0.0065735625, -0.028399143, 0.06347352, 0.0068353703, 0.0029352268, 0.009802015, -0.03498516, -0.04573186, 0.052193288, -0.02318271, 0.0813233, -0.050159283, -0.029143116, 0.004385591, 0.061123423, 0.008343888, -0.03514772, -0.0022204735, -0.019483222, 0.034856558, -0.035964698, 0.039332327, -0.030527556, -0.027816378, -0.028415024, -0.0037503787, 0.067192435, -0.018861812, 0.0101318555, -0.018039612, 0.053115103, -0.018329259, -0.046573132, -0.11817373, -0.03257061, 0.050309088, 0.031804174, 0.018976802, -0.0226638, -0.08759021, -0.038173307, -0.04612506, -0.072776444, -0.009059356, -0.02049697, 0.03738448, 0.01858798, 0.07336378, 0.00010577356, -0.013585844, -0.007857679, -0.060980856, -0.025185473, 0.017520785, -0.017940024, 0.00814232, -0.017396532, -0.020079888, -0.038534712, -0.054879185, 0.014338779, 0.032258503, 0.049125444, -0.015258698, -0.008578149, -0.009977055, 0.00048691922, 0.037752967, -0.035608854, -0.01044873, -0.02690444, -0.038794186, 0.00968896, -0.008792581, 0.0014637664, 0.06617355, 0.035733137, -0.015303109, 0.03604176, -0.024494842, 0.0360613, 0.034455147, 0.032420866, 0.014859369, -0.019173184, 0.07121669, 0.06237407, 0.043752633, 0.00050176715, -0.02695103, 0.016085962, 0.039019577, 0.06997477, 0.042752195, 0.011437008, -0.0045507136, 0.009138605, -0.016108584, 0.0044340217, 0.056161128, -0.0037258528, 0.028033204, 0.01615749, -0.0037737945, -0.038220733, -0.007683182, 0.016459746, -0.027550949, -0.03659914, 0.01453925, -0.08894217, 0.034170717, 0.07812462, 0.009253909, 0.004072164, -0.022045607, 0.013152617, -0.0032754848, 0.02828423, 0.030243117, 0.069175266, 0.025329696, -0.0037765831, -0.023651194, 0.0074150036, 0.0019427018, -0.009501996, 0.024450246, 0.02650216, 0.0018512008, -0.07205591, -0.037715375, -0.010633028, -0.052308016, -0.021726348, -0.03989294, 0.01897366, -0.023579245, 0.013368904, -0.03167263, 0.02285245, 0.0011101998, 0.07701884, 0.0098424405, 0.06934401, -0.037714064, -0.06487135, -0.033034, -0.017939214, 0.015933838, -0.008620415, -0.031752303, -0.0013771016, 0.024760041, -0.0407278, -0.001739277, 0.05734854, -0.036885366, -0.036178816, 0.09199307, -0.009435176, 0.013046756, 0.03294896, -0.011708637, 0.06314771, -0.07204531, -0.031142095, -0.026493968, -0.021137523, -0.018826192, -0.019225413, -0.03454975, 0.031863254, 0.018260192, 0.023154452, 0.049643308, -0.004950994, -0.04067718, -0.0016953726, -0.0075787287, -0.029662978, 0.041256793, -0.045994155, 0.027898153, -0.01834479, 0.0040992326, 0.005011376, -0.07244043, -0.009834766, 0.060436882, 0.032035522, 0.03154054, 0.027764117, -0.025347123, 0.008881899, -0.028873125, 0.04368815, 0.0420208, -0.037433352, 0.020340897, 0.037612647, 0.02257238, -0.062828936, -0.011043634, -0.00078356336, -0.0055557545, 0.005891466, 0.0347812, -0.00044118435, -0.016234957, -0.00044105318, 0.0061016013, -0.040603608, 0.060768805, -0.049695276, 0.012565908, -0.03958284, 0.028685449, 0.047944438, -0.013114668, -0.021886483, 0.016324487, -0.0018698417, -0.01566752, 0.06914219, -0.01686817, -0.020134272, 0.045933597, -0.0029879382, -0.023669463, 0.061855398, -0.007818121, 0.01122294, 0.0270429, -0.011470846, 0.079852946, -0.0096514635, 0.024244228, -0.0038649363, 0.027227614, -0.008899742, -0.036567856, -0.037570234, -0.009301449, -0.037451055, 0.0032769565, 0.04531888, -0.052479718, -0.082057126, 0.014333214, -0.0031147536, -0.03675208, -0.034505434, 0.0080362735, -0.037639897, 0.053857714, 0.046250254, 0.015970452, -0.0033167412, -0.044643547, 0.006164598, -0.04936485, 0.042411517, 0.024689043, 0.014214196, -0.011469853, 0.040061694, -0.010532354, 0.014155307, -0.012648687, -0.038097575, -0.00465958, 0.07236191, 0.05607798, -0.021266252, 0.02072716, -0.050752427, 0.04522972, 0.056116857, 0.038998585, 0.048427116, -0.0084286295, -0.02083299, -0.009333436, 0.018240292, 0.07700189, 0.026751067, 0.022996359, -0.00015188675, -0.0080190925, -0.03290913, 0.05662681, 0.0037033153, -0.011945093, -0.037654076, 0.024172544, 0.013991968, 0.0014496762, 0.0040987423, 0.0137514, -0.019176077, -0.02468889, 0.057817336, 0.00960155, -0.005842217, -0.010695956, 0.077245705, 0.022113454, 0.028442128, 0.052239887, -0.053479508, -0.008720564, -0.005013826, -0.034094118, 0.095873505, 0.014744527, 0.018044056, -0.029294278, -0.016875194, 0.038103845, -0.06048707, -0.013004376, 0.011846406, 0.036631163, -0.051293135, 0.014736944, -0.08294621, 0.004772536, -0.00086842454, -0.061981034, 0.014843105, -0.043987326, -0.03986193, 0.0028578944, -0.003303021, -0.019583376, -0.015006733, -0.011788964, -0.00852451, -0.021664975, 0.04637458, 0.027261913, 0.010843703, -0.041370697, 0.010178574, 0.009535152, 0.012286759, 0.029891776, 0.01859531, 0.023149459, 0.08561175, 0.016239477, 0.030251132, -0.0185829, -0.016931307, -0.037724927, 0.038730524, -0.01988481, 0.010195072, 0.0034290133, -0.04432525, -0.0101551125, -0.003631632, -0.009308001, -0.00020915573, -0.032804243, -0.024302796, 0.006352195, 0.07321011, -0.010080615, 0.015119003, -0.03857598, -0.056296356, -0.012763999, 0.063788444, -0.02393985, -0.0068670167, 0.0064762, 2.4799536e-05, 0.016241094, 0.0059768576, -0.021931943, -0.015225139, -0.013551132, 0.024987604, -0.0760138, -0.036031697, 0.03057968, 0.054513935, 0.03851996, -0.028926773, 0.0022744946, -0.043471612, -0.03571896, 0.025463436, 0.042255897, -0.011051896, 0.0002969534, 0.034200385, -0.053222306, 0.030184537, -0.035344213, -0.06328587, -0.011460325, -0.0012449401, -0.056811493, 0.033643205, -0.06851745, -0.017743956, -0.045912, -0.0041968515, -0.059316367, -0.00057225936, -0.030278966, -0.011019709, 0.025502346, 0.0029993793, 0.0017507572, -0.040308803, 0.022516131, 0.0023301102, -0.07340617, 0.027312009, -0.07518796, 0.026328357, -0.018056376, 0.043899722, 0.04591359, -0.005901131, -0.032551162, -0.0120589845, 0.0029816353, -0.030082887, 0.0039872522, -0.05010535, 0.017563688, 0.00593674, -0.045506243, -0.035508156, 0.012508543, 0.058185812, -0.009378965, -0.0026185971, 0.022348614, -0.026115825, -0.017831167, -0.014826945, 0.04451855, -0.018699571, -0.01122183, -0.05474404, -0.02468053, 0.003661165, 0.014639991, 0.0071215625, 0.035501696, 0.05460532, 0.045970585, -0.034972027, -0.019194327, -0.02318634, -0.028347174, 0.038711034, -0.07632577, 0.03048759, 0.021658385, 0.014550266, 0.0044268104, -0.03431152, 0.037028134, 0.019656748, 0.009410368, 0.03139573, -0.04557541, -0.03525484, -0.010141955, 0.025908595, -0.03948534, 0.057993673, -0.029252691, -0.08718715, 0.03398034, 0.0010825301, -0.034891732, -0.049304303, 0.00968243, -0.016249428, 0.015155405, -0.06198324, 0.07862992, -0.10798578, -0.015385354, -0.004516666, -0.06829362, 0.023623398, -0.024125347, 0.006282927, -0.007039721, 0.038392976, -0.039441265, 0.019500485, 0.0034804328, -0.059916027, 0.013149275, 0.024287438, -0.08641782, 0.0031457704, -0.041862767, -0.018808583, -0.042751208, 0.031302907, -0.0026946815, 0.02531959, -0.0067747287, -0.01873976, -0.03836008, 0.03382582, 0.0046534752, 0.017261764, -0.05272224, -0.01900113, -0.052270927, 0.0839039, -0.0023184612, -0.030702973, 0.026937986, 0.06391676, 0.011480133, 0.020584725, -0.0062598595, 0.01271743, -0.023733964, 0.03981261, 0.024233729, -0.030003157, -0.007563352, 0.01940493, -0.017332481, 0.044128664, -0.0011724766, -0.02113829, 0.052548014, -0.033863295, 0.055949163, -0.014369583, -0.008640694, 0.043073308, -0.001098384, -0.007095705, 0.065791495, -0.09256891, 0.05519638, -0.045391925, -0.0069478685, -0.004177882, -0.012355384, -0.031020103, -0.048190948, 0.05212499, -0.07414498, 0.064548865, -0.016638152, 0.026926804, 0.0014756493, -0.02014119, -0.02190092, -0.036945365, 0.046044525, -0.05363503, -0.015901528, -0.016490547, 0.0012935052, -0.051049758, -0.008628753, 0.040718395, 0.019815039, -0.028073907, -0.009422794, -0.037853874, 0.007922496, 0.030685287, -0.0009412508, 0.05551333, -0.016197251, 0.01569207, -0.057900663, 0.041183088, 0.009789326, 0.018127823, 0.07934595, 0.0035166724, -0.0015640527, -0.03142073, 0.088223524, 0.023906516, -0.07853496, 0.0104442565, 0.010905432, -0.09932864, 0.010596234, 0.038315773, 0.00039586204, -0.010046175, 0.073465064, 0.0153264105, -0.08517128, -0.0018262123, 0.027301036, -0.018402394, 0.004013523, -0.0105034085, -0.001341402, -0.01824875, -0.010383325, -0.04863643, 0.017799815, 0.03732077, -0.037525553, -0.0215328, -0.022066142, -0.005006448, 0.014444069, 0.021500066, -0.020598931, -0.051025216, -0.06812541, -0.012802869, 0.072049454, -0.08265385, 0.042500578, -8.8066656e-05, 0.022693133, 0.06208743, 0.015975993, -0.00011765499, 0.012839276, -0.0032348472, 0.057932302, 0.0062866127, -0.012643743, -0.023917068, 0.031217298, 0.018462872, 0.01525744, 0.057731006, -0.05268039, 0.0030033581, -0.03869319, -0.0044651716, -0.0045861835, -0.031123297, -0.023356102, 0.07135176, -0.0060185865, -0.006613575, -0.053194724, -0.0051010572, 0.07268133, 0.021660555, -0.039079696, -0.02493767, 0.0051013893, 0.0016195043, 0.033620223, -0.011462115, 0.0058179745, -0.024781898, 0.00014019643, 0.040869515, -0.029714746, -0.09083967, 0.021263838, 0.014767878, -0.038412258, -0.00458989, -0.0068195774, -0.049765334, 0.07517855, -0.014416679, 0.016311672, 0.04193517, -0.029584592, -0.027188782, 0.015341523, -0.015502754, 0.006159062, -0.009008726, -0.05205833, 0.035115488, -0.055725724, 0.006827898, -0.00430665, -0.0856061, 0.06986755, -0.013882157, 0.039940283, 2.9234623e-05, -0.045420878, -0.069370255, 0.033975903, 0.0110349525, 0.09110184, 0.028133249, -0.053107552, 0.007514273, -0.036405407, -0.045008898, -0.0073855454, -0.01786241, 0.0019845578, 0.038962264, 0.0065911, 0.044278424, -0.011684557, 0.0009857367, -0.012018812, -0.00547832, 0.021578016, -0.059486717, 0.0002462422, -0.0066670766, 0.0035011407, 0.013813678, 0.045274448, -0.050355017, 0.0063423286], [0.037408933, -0.047820058, -0.0034196004, -0.05372112, 0.052606188, 0.017480126, -0.0063432106, -0.0053097885, 0.02654455, 0.03956327, -0.029699078, 0.00032905157, 0.009869235, -0.013295764, -0.03775747, -0.0064385473, 0.017034212, 0.025019312, -0.007532975, 0.003799003, 0.012761336, 0.020098742, 0.017261531, 0.022680301, 0.0029933508, -0.019364413, 0.035644565, -0.06107179, -0.04067108, 0.029729621, -0.022521537, 0.019600064, -0.09476316, -0.056787193, -0.025103427, -0.041345406, -0.019605009, 0.04212608, 0.008146155, 0.026078636, 0.007786535, -0.009296619, -0.0048280363, -0.020524167, 0.050614983, 0.0059826407, 0.0026632673, 0.005083854, -0.06254868, -0.05213958, 0.033711787, -0.0477546, 0.07551009, -0.042321924, -0.02919957, -0.009004891, 0.07697745, 0.015143001, -0.035386678, -0.010288264, -0.013704099, 0.027461339, -0.021165073, 0.026263122, -0.007485177, -0.028727734, -0.030379415, 0.00910596, 0.044239428, 0.006232644, 0.01198594, -0.020780833, 0.058564566, -0.013602391, -0.01651701, -0.13285813, -0.04529075, 0.071072735, 0.036674805, -0.009941307, 0.0018966661, -0.08167189, -0.042398695, -0.04807798, -0.057211813, -0.0027087166, -0.03526489, 0.02055693, 0.027912963, 0.07222118, -0.0336576, 0.0028402237, -0.0018986426, -0.061665744, -0.023511063, 0.029255612, -0.025640039, 0.002030552, -0.012490583, -0.004714267, -0.06266687, -0.03916656, -0.006373957, 0.028259657, 0.06988279, 0.036423896, 0.0042356723, -0.018161453, 0.005251032, 0.013397659, -0.013611805, -0.0016877541, -0.03431037, -0.023531334, 0.0071231816, -0.012463034, 0.03273071, 0.057784792, 0.041182388, -0.024336653, 0.02751052, 0.022415563, 0.022298427, 0.010039597, 0.0043274276, 0.011936146, 0.0023567642, 0.058938596, 0.0774311, 0.07580927, 0.028709231, -0.018870289, 0.036128443, 0.034893665, 0.06816408, 0.047673102, 0.02319764, -0.016288651, 0.014597639, -0.006760575, 0.022082373, 0.035129435, -0.007899367, 0.01419147, -0.012902057, 0.012932501, -0.03546303, -0.031895313, 0.023259107, -0.02599701, -0.029912798, 0.011300255, -0.09615339, 0.04066552, 0.07686985, 0.011475225, 0.01665761, -0.014881794, 0.011358924, -0.0033665674, 0.012705795, 0.03671605, 0.057553556, -0.003450868, -0.0137605015, 0.012083003, -0.008535179, 0.01385153, 0.0014186318, 0.01786267, -0.006945186, 0.011193033, -0.06193308, -0.032519832, -0.010330629, -0.06796547, -0.017597511, -0.04116344, -0.033834916, -0.059251383, 0.0112543, -0.039138965, 0.025281625, 0.025447393, 0.055403102, -0.024152188, 0.055034034, -0.06480761, -0.046353333, -0.019411132, -0.018074285, 0.011342314, -0.006489135, -0.035579983, -0.01972933, 0.013770028, -0.034379147, 0.019319857, 0.045359872, -0.048549518, -0.024614226, 0.08454708, -0.0030881134, 0.030505856, 0.027221547, -0.012373234, 0.042340778, -0.075925365, -0.0246589, -0.028078703, -0.014675208, -0.03061765, -0.023450375, 0.00863174, 0.05504828, -0.00614469, 0.030240826, 0.06817562, 0.012624964, -0.034532554, 0.00092982996, -0.025002398, -0.019820316, 0.049000587, -0.026950277, -6.248291e-05, -0.0021047674, 0.019193882, -0.004560047, -0.06097993, 0.00082255254, 0.07817295, 0.023534246, 0.050538596, 0.014114849, -0.036960777, 0.021541812, 0.0013985183, 0.04794995, 0.021951664, -0.059586633, 0.014731106, 0.0337387, 0.022942724, -0.053076774, 0.00457561, -0.0012857419, -0.0102251535, -0.009570377, 0.028867327, -0.021280048, -0.039199598, -0.003880837, 0.0065321387, -0.040934194, 0.050700046, -0.06911357, 0.019092586, -0.027818108, 0.029947266, 0.042014748, -0.006409907, 0.0018863778, 0.011475971, -0.010476153, -0.030481903, 0.051953997, -0.035619963, -0.024021037, 0.030591352, -0.006204858, -0.013292268, 0.061334614, -0.010392525, -0.0142136235, 0.046938885, 0.00015920258, 0.06726701, 0.009881729, -0.0035922776, 0.010567856, 0.020166826, -0.027362421, -0.025603935, -0.030392734, 0.010432367, -0.0545353, -0.006432744, 0.049511287, -0.05541556, -0.08978901, 0.0054232273, 0.010512607, -0.040402994, -0.038179066, 0.013413637, -0.034157056, 0.056921802, 0.043584775, 0.019234095, -0.006675302, -0.06606348, 0.00413475, -0.0675714, 0.031254616, 0.018603837, 0.0141791, -0.0024756473, 0.024235636, -0.0037878144, 0.019325886, 0.004222294, -0.037963927, -0.0022319965, 0.06780218, 0.035405885, -0.03139529, 0.022325497, -0.021535268, 0.0010309152, 0.035253536, 0.03177928, 0.057032973, -0.015444153, -0.011125687, 0.012784688, -0.029546298, 0.08991227, 0.0040138667, -0.007147557, 0.0015905707, -0.017289924, -0.031850472, 0.05862823, -0.017189084, 0.01848521, -0.03636069, 0.052268513, -0.0018865027, 3.211723e-05, 0.015515397, 0.029095743, -0.04121964, -0.0388422, 0.04908826, 0.005805222, 0.009360262, 0.0047987774, 0.07419679, 0.032026477, 0.028360909, 0.046065647, -0.054801464, 0.010753127, 0.02099093, -0.021338893, 0.09733142, 0.009913079, 0.028790649, -0.026199648, -0.02240171, 0.04397028, -0.049903225, -0.0008452961, 0.00511143, 0.029129503, -0.056137167, 0.034854416, -0.0873517, 0.012144081, 0.0075966506, -0.050284322, 0.033919454, -0.051095285, -0.046566166, -0.008771218, -0.025097402, -0.01838773, 0.0033019434, -0.008898334, -0.004868446, -0.025879962, 0.031907443, 0.014702446, 0.015051224, -0.03594471, 0.00053528993, 0.023736855, 0.01863822, 0.01556736, -0.0019349261, 0.040526878, 0.08385315, 0.027127229, 0.02308442, -0.014449153, -0.007460896, -0.022196384, 0.05215773, -0.0029364554, -0.006294323, -0.0011661747, -0.03576305, 0.007005173, 0.027922872, -0.019069884, 0.0065003773, -0.02554658, -0.015926987, 0.03066041, 0.060280863, -0.00884016, 0.023381911, -0.026103985, -0.058417942, 0.008831565, 0.058634028, -0.033059254, -0.018609857, -0.0013111085, -0.032459717, 0.008922871, 0.00012554252, -0.04794774, -0.020460038, -0.028593015, 0.031380355, -0.06862722, -0.021754455, 0.0148065, 0.04292225, 0.04426441, 0.005114202, 0.015704138, -0.03884044, -0.023711735, 0.034717586, 0.021395648, -0.021834053, -0.014899512, 0.0234337, -0.04439605, 0.026182357, -0.052850876, -0.059273586, 0.013437234, 0.008237679, -0.04990016, 0.02532692, -0.08052606, 0.008320679, -0.064643435, -0.0292247, -0.07148959, -0.011784102, -0.01557344, -0.023806049, 0.009002211, -0.014652865, 0.008717081, -0.033417594, -0.002671214, -0.00068762444, -0.04845042, 0.022732157, -0.08047191, 0.024222188, -0.020972831, 0.055839207, 0.035862893, 0.017720496, -0.04020007, -0.01682267, -0.011573345, -0.04188623, 0.030062934, -0.06561808, 0.0007398886, -0.026188152, -0.038525302, -0.022422576, 0.00900688, 0.08100768, 0.0017803132, -0.018474039, 0.007520283, -0.007842753, -0.02261226, -0.040138043, 0.03143768, -0.033756793, -0.0036656873, -0.054111008, -0.011770572, 0.0055939006, 0.008090364, 0.00830368, 0.03587576, 0.038429502, 0.023483977, -0.03248036, -0.00076817634, -0.03024543, -0.026744498, 0.045920737, -0.034004625, 0.033125006, 0.025552066, 0.0056548323, 0.0030224978, -0.025934812, 0.018050868, 0.01285696, -0.0062171575, 0.023992063, -0.030086141, -0.03912708, -0.05524911, 0.047725376, -0.018091327, 0.052535586, -0.03480978, -0.06484253, -0.0006865634, 0.009394308, -0.06787697, -0.03711805, 0.0074724066, -0.024799325, 0.038461193, -0.037216596, 0.06785343, -0.07350149, 0.022158425, -0.012961068, -0.053955816, 0.037212357, -0.0147288805, 0.0274685, -0.0018220581, 0.048867647, -0.020844545, 0.03434748, -0.0062744915, -0.0335427, -0.008295709, 0.028815053, -0.076976225, 0.034366913, -0.030629674, -0.015479316, -0.04745663, 0.051494326, 0.0038072502, 0.011936667, -0.019763118, -0.0049714656, -0.039229102, 0.026300164, 0.02550984, 0.007086616, -0.046221852, -0.006412145, -0.048882272, 0.10270072, -0.00071955717, -0.052447226, -0.009365011, 0.07400576, -0.0036078705, -0.0013080924, -0.006551573, 0.018752256, -0.032565486, 0.053315695, -0.014108104, -0.034862798, -0.03510968, 0.004374773, 0.0070778737, 0.04404155, -0.016149553, 0.013303175, 0.07092196, -0.031485874, 0.0489268, -0.04181442, -0.014568487, 0.057688825, -0.017329523, -0.03934629, 0.081124865, -0.073600404, 0.036883753, -0.043815117, -0.021391174, -0.011811617, -0.032829694, -0.009437782, -0.062106896, 0.04739052, -0.059612278, 0.04095798, -0.026175637, 0.04554948, 0.006648548, -0.010879573, -0.025135962, 0.0054670204, 0.005240811, -0.036778875, -0.03955242, 0.021835102, -5.022785e-05, -0.06160297, -0.014455816, 0.029518332, 0.0007582171, -0.0026328552, 0.009401512, -0.024406772, 0.008736568, 0.030955613, -0.0049137925, 0.09281259, 0.0038203292, 0.015487998, -0.058685526, 0.0653072, 0.020008089, 0.028542614, 0.063267395, 0.006020049, -0.03475062, -0.03209986, 0.084015325, 0.014645297, -0.06277902, 0.003085248, 0.022374198, -0.068229675, 0.022153517, 0.03448057, -0.030131316, -0.0020958267, 0.07014537, 0.019564426, -0.084438786, 0.020668218, 0.043725472, -0.020839093, -0.02274508, -0.01932832, -0.028166434, -0.016635269, 0.0033086226, -0.052748214, 0.008176954, 0.053680837, -0.035657547, -0.0141583495, -0.028470764, -0.017621368, 0.0035200391, 0.018026544, -0.01731577, -0.040621094, -0.06083414, 0.01390163, 0.07121761, -0.064307146, 0.052853014, -0.018669225, 0.029086906, 0.060825836, 0.024286984, 0.039023932, 0.017320368, -0.04071935, 0.04242379, 0.007871904, 0.005223725, -0.015299776, 0.0061480226, 0.012346293, 0.0147641925, 0.06506518, -0.024281671, -0.02216104, -0.046219338, -0.020680381, -0.0066815163, -0.020058049, -0.032076634, 0.06496326, -0.006364667, -0.04359371, -0.06535083, -0.021131318, 0.06607596, 0.019617956, -0.026725749, -0.010749917, 0.004542683, -0.0072607533, 0.033530492, -0.037921138, 0.0011212397, -0.009291517, 0.033473786, 0.043970976, -0.030985119, -0.07713241, 0.01840475, 0.011467913, -0.02839871, -0.004942087, -0.023698403, -0.014050632, 0.0921894, -0.004206678, 0.0032954542, 0.040612694, -0.036114197, -0.027048623, 0.010851619, -0.037619486, -0.0033824244, 0.008003312, -0.027466811, 0.053822145, -0.057304114, 0.03502779, -0.0008228949, -0.075297296, 0.068157226, -0.018764831, 0.041737504, -0.018785773, -0.08954464, -0.075991176, 0.0061312416, 0.012244951, 0.11438735, 0.013535321, -0.05712589, 0.00421147, -0.02374936, -6.721783e-05, 0.031514917, -0.020184513, 0.015759604, 0.028632835, -0.007585182, 0.029806303, -0.039464764, -0.021518823, -0.018926306, 0.0043636025, 0.027688423, -0.04745522, 0.007623723, -0.019160056, 0.0102957925, -0.016086534, 0.0030399244, -0.025126925, 0.004205378]]\n"
     ]
    }
   ],
   "source": [
    "# prompt: give me google gemini code to embed text\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Assuming you have a secret named 'GEMINI_API_KEY' set in Colab secrets\n",
    "# Replace with your actual secret name if different\n",
    "GEMINI_API_KEY = ''\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "  print(\"GEMINI_API_KEY not found in Colab secrets. Please set it.\")\n",
    "else:\n",
    "  genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "  # Example of embedding text\n",
    "  text_to_embed = \"This is a test sentence for embedding.\"\n",
    "  result = genai.embed_content(model=\"models/embedding-001\", content=text_to_embed)\n",
    "\n",
    "  # The embedding is in result['embedding']\n",
    "  embedding = result['embedding']\n",
    "  print(\"Embedding:\", embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fresh: 0/424 embedded chunks\n",
      "Embedded 50/424 chunks. 374 remaining.\n",
      "Embedded 100/424 chunks. 324 remaining.\n",
      "Embedded 150/424 chunks. 274 remaining.\n",
      "Embedded 200/424 chunks. 224 remaining.\n",
      "Embedded 250/424 chunks. 174 remaining.\n",
      "Embedded 300/424 chunks. 124 remaining.\n",
      "Embedded 350/424 chunks. 74 remaining.\n",
      "Embedded 400/424 chunks. 24 remaining.\n",
      "Embedded 424/424 chunks. 0 remaining.\n",
      "✅ All done. Embeddings saved to C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\\embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import tiktoken             # pip install tiktoken\n",
    "import google.generativeai as genai  # pip install google-generativeai\n",
    "from google.api_core.exceptions import ResourceExhausted, GoogleAPICallError\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIGURATION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "BASE_DIR      = r\"C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\"\n",
    "NORMAL_FILE   = os.path.join(BASE_DIR, \"normalized_docs2.json\")\n",
    "EMBED_FILE    = os.path.join(BASE_DIR, \"embeddings.npz\")\n",
    "META_FILE     = os.path.join(BASE_DIR, \"metadata.json\")\n",
    "\n",
    "GEN_API_KEY   = os.getenv(\"GOOGLE_API_KEY\") \n",
    "if not GEN_API_KEY:\n",
    "    raise RuntimeError(\"Please set the GOOGLE_API_KEY environment variable\")\n",
    "\n",
    "MODEL         = \"models/embedding-001\"\n",
    "BATCH_SIZE    = 50\n",
    "MAX_RPM       = 1500\n",
    "SLEEP_BETWEEN = 60.0 / MAX_RPM   # ~0.04s between calls\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# SETUP\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "genai.configure(api_key=GEN_API_KEY)\n",
    "\n",
    "with open(NORMAL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    docs = json.load(f)\n",
    "texts    = [d[\"page_content\"] for d in docs]\n",
    "metadata = [d[\"metadata\"]     for d in docs]\n",
    "total    = len(texts)\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Resume if possible\n",
    "if os.path.exists(EMBED_FILE) and os.path.exists(META_FILE):\n",
    "    data      = np.load(EMBED_FILE)\n",
    "    embs      = data[\"arr_0\"].tolist()\n",
    "    start_idx = len(embs)\n",
    "    print(f\"Resuming from {start_idx}/{total} embedded chunks\")\n",
    "else:\n",
    "    embs      = []\n",
    "    start_idx = 0\n",
    "    with open(META_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False)\n",
    "    print(f\"Starting fresh: 0/{total} embedded chunks\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# EMBEDDING LOOP\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "for i in range(start_idx, total, BATCH_SIZE):\n",
    "    raw_batch = texts[i : i + BATCH_SIZE]\n",
    "    # strip any bad UTF-8 surrogates\n",
    "    batch_texts = [t.encode('utf-8', 'ignore').decode('utf-8') for t in raw_batch]\n",
    "    tries = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            resp = genai.embed_content(model=MODEL, content=batch_texts)\n",
    "            # fix: handle whichever key is present\n",
    "            if \"embeddings\" in resp:\n",
    "                vectors = resp[\"embeddings\"]\n",
    "            elif \"data\" in resp and isinstance(resp[\"data\"], list):\n",
    "                vectors = [item[\"embedding\"] for item in resp[\"data\"]]\n",
    "            elif \"embedding\" in resp:\n",
    "                vectors = resp[\"embedding\"]\n",
    "            else:\n",
    "                raise KeyError(f\"No embeddings in response keys={list(resp.keys())}\")\n",
    "            embs.extend(vectors)\n",
    "            break\n",
    "\n",
    "        except ResourceExhausted:\n",
    "            print(\"429 rate limit hit; sleeping 60s…\")\n",
    "            time.sleep(60)\n",
    "\n",
    "        except GoogleAPICallError as e:\n",
    "            tries += 1\n",
    "            if tries <= 3:\n",
    "                wait = 5 * tries\n",
    "                print(f\"API error, retry {tries}/3 in {wait}s…\", e)\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                print(f\"Skipping batch at index {i} after 3 failures.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error at batch {i}: {e}\")\n",
    "            raise\n",
    "\n",
    "    # incremental save\n",
    "    arr = np.array(embs, dtype=\"float32\")\n",
    "    np.savez_compressed(EMBED_FILE, arr)\n",
    "    done = len(embs)\n",
    "    left = total - done\n",
    "    print(f\"Embedded {done}/{total} chunks. {left} remaining.\")\n",
    "    time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "print(\"✅ All done. Embeddings saved to\", EMBED_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba06faf",
   "metadata": {},
   "source": [
    "# retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e66a1d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index contains 424 vectors of dimension 768\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "\n",
    "# 1. Load embeddings and metadata\n",
    "emb_data = np.load(r\"C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\\embeddings.npz\")\n",
    "embeddings = emb_data[\"arr_0\"]               # shape (N, D)\n",
    "with open(r\"C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\\metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)                   # list of length N\n",
    "\n",
    "# 2. Build FAISS index (L2 distance)\n",
    "D = embeddings.shape[1]                       # embedding dimension\n",
    "index = faiss.IndexFlatL2(D)                  # exact L2\n",
    "index.add(embeddings)                         # add all N vectors\n",
    "print(f\"FAISS index contains {index.ntotal} vectors of dimension {D}\")\n",
    "\n",
    "# 3. (Optional) Normalize for cosine similarity:\n",
    "# faiss.normalize_L2(embeddings)\n",
    "# index = faiss.IndexFlatIP(D)\n",
    "# index.add(embeddings)\n",
    "\n",
    "# 4. Retrieval helper\n",
    "def retrieve(query_emb: np.ndarray, k: int = 5):\n",
    "    \"\"\"\n",
    "    query_emb: 1-D array of length D\n",
    "    returns: list of dicts {metadata, distance}\n",
    "    \"\"\"\n",
    "    # ensure shape (1, D)\n",
    "    q = query_emb.reshape(1, -1).astype(\"float32\")\n",
    "    distances, indices = index.search(q, k)\n",
    "    \n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        entry = metadata[idx].copy()\n",
    "        entry[\"score\"] = float(dist)\n",
    "        results.append(entry)\n",
    "    return results\n",
    "\n",
    "# 5. Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppose you have a query embedding already:\n",
    "    # query_emb = <your embedding pipeline>\n",
    "    # hits = retrieve(query_emb, k=5)\n",
    "    # for h in hits:\n",
    "    #     print(h[\"source_url\"], \"→\", h[\"score\"])\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 424 embeddings of dimension 768\n",
      "\n",
      "Top 5 results:\n",
      "1. (score: 0.47) https://tds.s-anand.net/llm-website-scraping.md\n",
      "   LLM Website Scraping...\n",
      "\n",
      "2. (score: 0.49) https://tds.s-anand.net/data-sourcing.md\n",
      "   Data Sourcing Before you do any kind of data science, you obviously have to get the data to be able to analyze it, visualize it, narrate it, and deploy it. And what we are going to cover in this modul...\n",
      "\n",
      "3. (score: 0.50) https://tds.s-anand.net/scraping-live-sessions.md\n",
      "   Scraping: Live Sessions Fundamentals of web scraping with urllib and BeautifulSoup Intermediate web scraping use of cookies XML intro and scraping...\n",
      "\n",
      "4. (score: 0.54) https://tds.s-anand.net/scraping-emarketer.md\n",
      "   Scraping emarketer In this live scraping session, we explore a real-life scenario where Straive had to scrape data from emarketer.com for a demo. This is a fairly realistic and representative way of h...\n",
      "\n",
      "5. (score: 0.58) https://tds.s-anand.net/scraping-with-google-sheets.md\n",
      "   Scraping with Google Sheets You'll learn how to , covering: Import HTML Formula**: Use =IMPORTHTML(URL, \"query\", index) to fetch tables or lists from a web page. Granting Access**: Allow access for fo...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "BASE_DIR       = r\"C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\"\n",
    "NORMAL_FILE    = os.path.join(BASE_DIR, \"normalized_docs2.json\")\n",
    "EMBED_FILE     = os.path.join(BASE_DIR, \"embeddings.npz\")\n",
    "META_FILE      = os.path.join(BASE_DIR, \"metadata.json\")\n",
    "\n",
    "GEN_API_KEY    = os.getenv(\"GOOGLE_API_KEY\")\n",
    "MODEL          = \"models/embedding-001\"\n",
    "TOP_K          = 5\n",
    "\n",
    "# ─── INITIALIZE API ───────────────────────────────────────────────────────────\n",
    "genai.configure(api_key=GEN_API_KEY)\n",
    "\n",
    "# ─── LOAD DATA ────────────────────────────────────────────────────────────────\n",
    "# Load chunk texts + metadata\n",
    "with open(NORMAL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    docs = json.load(f)\n",
    "texts    = [d[\"page_content\"] for d in docs]\n",
    "metas    = [d[\"metadata\"]     for d in docs]\n",
    "\n",
    "# Load embeddings\n",
    "emb_data  = np.load(EMBED_FILE)\n",
    "embeddings = emb_data[\"arr_0\"].astype(\"float32\")\n",
    "\n",
    "# ─── BUILD FAISS INDEX ─────────────────────────────────────────────────────────\n",
    "dim   = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "print(f\"Loaded {index.ntotal} embeddings of dimension {dim}\")\n",
    "\n",
    "# ─── QUERY EMBEDDING ──────────────────────────────────────────────────────────\n",
    "def embed_query(text: str) -> np.ndarray:\n",
    "    resp = genai.embed_content(model=MODEL, content=[text])\n",
    "    # handle possible fields\n",
    "    if \"embeddings\" in resp:\n",
    "        vec = resp[\"embeddings\"][0]\n",
    "    elif \"data\" in resp:\n",
    "        vec = resp[\"data\"][0][\"embedding\"]\n",
    "    elif \"embedding\" in resp:\n",
    "        vec = resp[\"embedding\"]\n",
    "    else:\n",
    "        raise KeyError(f\"No embedding in response keys={resp.keys()}\")\n",
    "    return np.array(vec, dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "# ─── RETRIEVE FUNCTION ────────────────────────────────────────────────────────\n",
    "def retrieve(question: str, k: int = TOP_K):\n",
    "    # 1) embed the question\n",
    "    q_vec = embed_query(question)\n",
    "    # 2) search index\n",
    "    dists, idxs = index.search(q_vec, k)\n",
    "    # 3) assemble results\n",
    "    results = []\n",
    "    for dist, idx in zip(dists[0], idxs[0]):\n",
    "        results.append({\n",
    "            \"chunk_text\": texts[idx],\n",
    "            **metas[idx],\n",
    "            \"score\": float(dist)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ─── DEMO ─────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    question = input(\"Enter your question: \").strip()\n",
    "    hits = retrieve(question)\n",
    "    print(f\"\\nTop {len(hits)} results:\")\n",
    "    for i, h in enumerate(hits, 1):\n",
    "        url = h.get(\"source_url\") or h.get(\"url\") or \"<no-url>\"\n",
    "        print(f\"{i}. (score: {h['score']:.2f}) {url}\\n   {h['chunk_text'][:200].replace(chr(10),' ')}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b294aed",
   "metadata": {},
   "source": [
    "# prompting and llm calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7baa6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 128\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    127\u001b[0m     q \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi want to learn docker and podman what to choose \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m--> 128\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLinks:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 119\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m    117\u001b[0m text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# 5) Parse JSON out\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import google.generativeai as genai\n",
    "from google.api_core.exceptions import ResourceExhausted, GoogleAPICallError\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIGURATION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "BASE_DIR       = r\"C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\"\n",
    "NORMAL_FILE    = os.path.join(BASE_DIR, \"normalized_docs2.json\")\n",
    "EMBED_FILE     = os.path.join(BASE_DIR, \"embeddings.npz\")\n",
    "META_FILE      = os.path.join(BASE_DIR, \"metadata.json\")\n",
    "\n",
    "QUESTION_MODEL = \"models/embedding-001\"\n",
    "CHAT_MODEL     = \"gemini-1.5-flash\"  # alias for the latest Gemini 1.5 Flash\n",
    "TOP_K          = 5\n",
    "\n",
    "# API Key\n",
    "GEN_API_KEY    = \"\"\n",
    "if not GEN_API_KEY:\n",
    "    raise RuntimeError(\"Set the GOOGLE_API_KEY environment variable\")\n",
    "genai.configure(api_key=GEN_API_KEY)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# LOAD DATA & INDEX\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) normalized docs\n",
    "with open(NORMAL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    docs = json.load(f)\n",
    "texts    = [d[\"page_content\"] for d in docs]\n",
    "metas    = [d[\"metadata\"]     for d in docs]\n",
    "\n",
    "# 2) embeddings\n",
    "arr       = np.load(EMBED_FILE)[\"arr_0\"].astype(\"float32\")\n",
    "dim       = arr.shape[1]\n",
    "\n",
    "# 3) FAISS\n",
    "index     = faiss.IndexFlatL2(dim)\n",
    "index.add(arr)\n",
    "with open(META_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# QUERY EMBEDDING & RETRIEVAL\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def embed_query(text: str) -> np.ndarray:\n",
    "    resp = genai.embed_content(model=QUESTION_MODEL, content=[text])\n",
    "    # extract embedding from whichever key is present\n",
    "    if \"embeddings\" in resp:\n",
    "        vec = resp[\"embeddings\"][0]\n",
    "    elif \"data\" in resp:\n",
    "        vec = resp[\"data\"][0][\"embedding\"]\n",
    "    elif \"embedding\" in resp:\n",
    "        vec = resp[\"embedding\"]\n",
    "    else:\n",
    "        raise KeyError(f\"No embedding in response: {resp.keys()}\")\n",
    "    return np.array(vec, dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "def retrieve(question: str, k: int = TOP_K):\n",
    "    q_vec = embed_query(question)\n",
    "    dists, idxs = index.search(q_vec, k)\n",
    "    hits = []\n",
    "    for dist, idx in zip(dists[0], idxs[0]):\n",
    "        hit = {\n",
    "            \"chunk_text\": texts[idx],\n",
    "            **(metas[idx]),\n",
    "            \"score\": float(dist)\n",
    "        }\n",
    "        hits.append(hit)\n",
    "    return hits\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# PROMPT CONSTRUCTION & GENERATION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def generate_answer(question: str):\n",
    "    # 1) Retrieve top-k\n",
    "    hits = retrieve(question)\n",
    "\n",
    "    # 2) Build context block\n",
    "    ctx_lines = []\n",
    "    for i, h in enumerate(hits, 1):\n",
    "        url = h.get(\"source_url\") or h.get(\"url\")\n",
    "        snippet = h[\"chunk_text\"].replace(\"\\n\", \" \")\n",
    "        snippet = snippet if len(snippet) < 200 else snippet[:197] + \"...\"\n",
    "        ctx_lines.append(f\"[{i}] \\\"{snippet}\\\" (Source: {url})\")\n",
    "    context = \"\\n\".join(ctx_lines)\n",
    "\n",
    "    # 3) Build chat messages\n",
    "    system = (\n",
    "        \"You are the IITM TDS Virtual TA. \"\n",
    "        \"Use the provided context snippets and their exact source URLs to answer \"\n",
    "        \"the student’s question. Cite all sources by URL.\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"Produce a JSON object with two fields:\\n\"\n",
    "        \"  \\\"answer\\\": A concise answer.\\n\"\n",
    "        \"  \\\"links\\\": A list of exactly these source URLs in the order you used them.\\n\"\n",
    "        \"Do not include any extra keys.\"\n",
    "    )\n",
    "\n",
    "    # 4) Call Gemini 1.5 Flash\n",
    "    gen_model = genai.GenerativeModel( CHAT_MODEL,system_instruction=(system))\n",
    "    \n",
    "    try:\n",
    "        response = gen_model.generate_content(user)\n",
    "    except ResourceExhausted:\n",
    "        # rate-limited—back off and retry once\n",
    "        time.sleep(60)\n",
    "        response = gen_model.generate_content(user)\n",
    "\n",
    "    text = response.text.strip()\n",
    "    # 5) Parse JSON out\n",
    "    return json.loads(text)\n",
    "\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    q = (\"i want to learn docker and podman what to choose \").strip()\n",
    "    result = generate_answer(q)\n",
    "    print(\"\\nAnswer:\\n\", result[\"answer\"])\n",
    "    print(\"\\nLinks:\")\n",
    "    for link in result[\"links\"]:\n",
    "        print(\" -\", link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d67b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adsdasaff\n",
      "```json\n",
      "{\n",
      "  \"answer\": \"The deadline for GA2 has been changed multiple times.  Initially it was January 26th, then February 2nd, and finally it's been changed to May 25th, 2025.  Please refer to the most recent announcements for the most up-to-date information.\",\n",
      "  \"links\": [\n",
      "    {\n",
      "      \"url\": \"https://tds.s-anand.net/README.md\",\n",
      "      \"text\": \"README file containing initial GA2 date and other updates.\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://discourse.onlinedegree.iitm.ac.in/t/revised-dates-tds-jan-2025/168506/1\",\n",
      "      \"text\": \"Discourse forum thread with revised dates.\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://discourse.onlinedegree.iitm.ac.in/t/ga2-deadline/165142/1\",\n",
      "      \"text\": \"Discourse forum thread discussing conflicting GA2 deadlines.\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://tds.s-anand.net/README.md\",\n",
      "      \"text\": \"README file containing project and assignment deadlines.\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://discourse.onlinedegree.iitm.ac.in/t/project-2-and-week-6-assignment/168303/1\",\n",
      "      \"text\": \"Discourse forum thread discussing delays and submission date changes.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import faiss\n",
    "import google.generativeai as genai\n",
    "from google.api_core.exceptions import ResourceExhausted, GoogleAPICallError\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIGURATION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "BASE_DIR       = r\"C:\\Users\\adavy\\Desktop\\TDS_P1\\dataset_test\"\n",
    "NORMAL_FILE    = os.path.join(BASE_DIR, \"normalized_docs2.json\")\n",
    "EMBED_FILE     = os.path.join(BASE_DIR, \"embeddings.npz\")\n",
    "META_FILE      = os.path.join(BASE_DIR, \"metadata.json\")\n",
    "\n",
    "QUESTION_MODEL = \"models/embedding-001\"\n",
    "CHAT_MODEL     = \"gemini-1.5-flash\"\n",
    "TOP_K          = 5\n",
    "\n",
    "# Load your API key from env or hard-code (for testing)\n",
    "GEN_API_KEY    = os.getenv(\"GOOGLE_API_KEY\") or \"\"\n",
    "if not GEN_API_KEY:\n",
    "    raise RuntimeError(\"Set the GOOGLE_API_KEY env var\")\n",
    "genai.configure(api_key=GEN_API_KEY)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# LOAD & BUILD INDEX\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Load normalized docs\n",
    "with open(NORMAL_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    docs = json.load(f)\n",
    "texts = [d[\"page_content\"] for d in docs]\n",
    "metas = [d[\"metadata\"]     for d in docs]\n",
    "\n",
    "# Load embeddings\n",
    "arr = np.load(EMBED_FILE)[\"arr_0\"].astype(\"float32\")\n",
    "dim = arr.shape[1]\n",
    "\n",
    "# Build FAISS\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(arr)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# EMBEDDING & RETRIEVE FUNCTIONS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def embed_query(text: str) -> np.ndarray:\n",
    "    resp = genai.embed_content(model=QUESTION_MODEL, content=[text])\n",
    "    if \"embeddings\" in resp:\n",
    "        vec = resp[\"embeddings\"][0]\n",
    "    elif \"data\" in resp:\n",
    "        vec = resp[\"data\"][0][\"embedding\"]\n",
    "    elif \"embedding\" in resp:\n",
    "        vec = resp[\"embedding\"]\n",
    "    else:\n",
    "        raise KeyError(f\"No embedding in response: {resp.keys()}\")\n",
    "    return np.array(vec, dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "def retrieve(question: str, k: int = TOP_K):\n",
    "    q_vec = embed_query(question)\n",
    "    dists, idxs = index.search(q_vec, k)\n",
    "    hits = []\n",
    "    for dist, idx in zip(dists[0], idxs[0]):\n",
    "        hits.append({\n",
    "            \"chunk_text\": texts[idx],\n",
    "            **metas[idx],\n",
    "            \"score\": float(dist)\n",
    "        })\n",
    "    return hits\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# PROMPT + CHAT + PARSE\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def generate_answer(question: str):\n",
    "    # 1) retrieve top-K\n",
    "    hits = retrieve(question)\n",
    "    \n",
    "    # 2) assemble context snippets\n",
    "    context_lines = []\n",
    "    for i, h in enumerate(hits, 1):\n",
    "        url = h.get(\"source_url\") or h.get(\"url\", \"\")\n",
    "        snippet = h[\"chunk_text\"].replace(\"\\n\", \" \")\n",
    "        snippet = snippet if len(snippet) <= 200 else snippet[:197] + \"...\"\n",
    "        context_lines.append(f\"[{i}] \\\"{snippet}\\\" (Source: {url})\")\n",
    "    context_block = \"\\n\".join(context_lines)\n",
    "\n",
    "    # 3) strict system + user prompts\n",
    "    system = (\n",
    "        \"You are an amazing professor of applications of data science tools with experience of 20+ years. You are replying to students on the Discourse forum to solve their problems using the context. \"\n",
    "        \"You have also been provided with context containing FAQs and course content to use as reference. \"\n",
    "        \"add all the links provided in context as it is return all the links you can make the content short but mention all links and you can leave text blank\"\n",
    "        \"You must respond **only** with a valid JSON object, NO EXTRA TEXT — only JSON format, nothing extra (not even ```json or any prefix/suffix).\\n\"\n",
    "        \"Schema:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"answer\": string,\\n'\n",
    "        '  \"links\": [\\n'\n",
    "        '    {\\n'\n",
    "        '      \"url\": \"https://........\",\\n'\n",
    "        '      \"text\": \"description for link.\"\\n'\n",
    "        '    },\\n'\n",
    "        '    {\\n'\n",
    "        '      \"url\": \"https://discourse.......\" ,\\n'\n",
    "        '      \"text\": \"description for link\"\\n'\n",
    "        '    }\\n'\n",
    "        '  ]\\n'\n",
    "        \"}\"\n",
    "    )\n",
    "    user = (\n",
    "        f\"Context:\\n{context_block}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"Always output exactly one JSON object following the schema above.\"\n",
    "        \n",
    "    )\n",
    "\n",
    "    # 4) start chat session with system instruction\n",
    "\n",
    "    gen_model = genai.GenerativeModel( CHAT_MODEL,system_instruction=(system))\n",
    "    \n",
    "    try:\n",
    "        response = gen_model.generate_content(user)\n",
    "    except ResourceExhausted:\n",
    "        time.sleep(60)\n",
    "        response = gen_model.generate_content(user)\n",
    "\n",
    "    text = response.text.strip()\n",
    "    # 6) parse and return JSON\n",
    "    return (text)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    q = (\"when is graded assignment ga 2 deadline \").strip()\n",
    "    out = generate_answer(q)\n",
    "\n",
    "    print(\"adsdasaff\")\n",
    "    print(out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
